{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training autoencoders on cytokine data\n",
    "To run this notebook, you need:\n",
    " - Tensorflow Python package (tested with version 2.0)\n",
    " - Preprocessed cytokine time series data in the ``data/processed/`` folder. The list of necessary files can be found a few cells below (testing and training on separate experiments). \n",
    "\n",
    "\n",
    "## Motivation\n",
    "The goal is to see if we can recover the latent space from an unsupervised approach, and to show that the supervised approach with cytokine integrals rather than concentrations is probably slightly better. To achieve this, we use a simple two-layer autoencoder with a bottleneck. We use the Keras module in Tensorflow. \n",
    "\n",
    "## Import packages, define useful classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import psutil, pickle, json\n",
    "import os, sys\n",
    "main_dir_path = os.path.abspath('../')\n",
    "if main_dir_path not in sys.path:\n",
    "    sys.path.insert(0, main_dir_path)\n",
    "\n",
    "# Processing: min-max scaling of the data\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Neural networks: keras for autoencoder\n",
    "from tensorflow import keras as ks\n",
    "import tensorflow as tf\n",
    "from utils.autoencoder_models import Autoencoder, Autoencoder_regul, save_autoencoder, load_autoencoder\n",
    "import random as python_random\n",
    "\n",
    "# Custom scripts\n",
    "import utils.custom_pandas as cpd\n",
    "from metrics.mi_time_window import compute_mi_timecourse\n",
    "from metrics.discrete_continuous_info import discrete_continuous_info_fast\n",
    "from ltspcyt.scripts.neural_network import import_WT_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot parameters for Science\n",
    "plt.rcParams[\"figure.figsize\"] = (2.5, 2.25)\n",
    "plt.rcParams[\"axes.labelsize\"] = 8.\n",
    "plt.rcParams[\"legend.fontsize\"] = 8.\n",
    "plt.rcParams[\"axes.labelpad\"] = 0.5\n",
    "plt.rcParams[\"xtick.labelsize\"] = 7.\n",
    "plt.rcParams[\"ytick.labelsize\"] = 7.\n",
    "plt.rcParams[\"legend.title_fontsize\"] = 8.\n",
    "plt.rcParams[\"axes.titlesize\"] = 8.\n",
    "plt.rcParams[\"font.size\"] = 8.\n",
    "\n",
    "# For larger display of small graphs in the notebook\n",
    "plt.rcParams['figure.dpi'] = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colors for the different kinds of latent spaces\n",
    "different_ls_colors = {\n",
    "    \"Ag. classifier\": mpl.colors.to_rgba(\"maroon\"),  # already set by main text figures: choose between [\"goldenrod\", \"maroon\"]\n",
    "    \"Autoencoder\": sns.color_palette(\"hls\", 3)[1], \n",
    "    \"PCA\": sns.color_palette(\"hls\", 3)[2]\n",
    "}\n",
    "sns.palplot(different_ls_colors.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU count for multiprocessing with the right number of jobs\n",
    "cpu_count = psutil.cpu_count(logical=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder class\n",
    "Using the Keras model API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data, select training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data and check the normalization of each feature. \n",
    "df = import_WT_output(folder=os.path.join(main_dir_path, \"data\", \"processed\"))\n",
    "df = df.loc[df.index.isin([\"1uM\", \"100nM\", \"10nM\", \"1nM\"], level=\"Concentration\")]\n",
    "df = df.loc[:, df.columns.isin([\"IFNg\", \"IL-17A\", \"IL-2\", \"IL-6\", \"TNFa\"], level=\"Cytokine\")]\n",
    "\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize train data and scale test data with same boundaries\n",
    "train_sets = [\n",
    "    'PeptideComparison_2',\n",
    "    'PeptideComparison_3',\n",
    "    'PeptideComparison_4',\n",
    "    'PeptideComparison_5',\n",
    "    'TCellNumber_1', \n",
    "    'Activation_1'\n",
    "]\n",
    "training_peps = [\"N4\", \"Q4\", \"T4\", \"V4\", \"G4\", \"E1\"]\n",
    "df_train = df.loc[df.index.isin(train_sets, level=\"Data\")]\n",
    "# Select only integrals\n",
    "df_train = df_train.xs(\"integral\", level=\"Feature\", axis=1)\n",
    "# Select only the training peptides\n",
    "df_train = df_train.loc[df_train.index.isin(training_peps, level=\"Peptide\")]\n",
    "\n",
    "test_sets = [\n",
    "    'Activation_3', \n",
    "    'TCellNumber_3', \n",
    "    'PeptideComparison_1',\n",
    "    'PeptideComparison_7',\n",
    "    'PeptideComparison_8',\n",
    "    'PeptideComparison_9',\n",
    "    'HighMI_1-1', 'HighMI_1-2',\n",
    "    'HighMI_1-3', 'HighMI_1-4'\n",
    "]\n",
    "\n",
    "df_test = df.loc[df.index.isin(test_sets, level=\"Data\")]\n",
    "df_test = df_test.xs(\"integral\", level=\"Feature\", axis=1)\n",
    "# Select only the training peptides\n",
    "df_test = df_test.loc[df_test.index.isin(training_peps, level=\"Peptide\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "df_min, df_max = df_train.min(axis=0), df_train.max(axis=0)\n",
    "df_train_norm = (df_train - df_min) / (df_max - df_min)\n",
    "df_test_norm = (df_test - df_min) / (df_max - df_min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train an autoencoder\n",
    "Optimal results can apparently be obtained by using only the time inverval 20 - 70 hours. \n",
    "But let me first try with the full time courses. \n",
    "\n",
    "Apply regularization to the first layer, to try to force the autoencoder out of the linear, PCA regime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(hist, df_train, df_test):\n",
    "    # Calculate R2\n",
    "    r2_history = 1.0 - np.asarray(hist.history['loss'])*df_train.size / ((df_train - df_train.mean(axis=0))**2).sum().sum()\n",
    "    val_r2_history = 1.0 - np.asarray(hist.history['val_loss'])*df_train.size / ((df_test - df_test.mean(axis=0))**2).sum().sum()\n",
    "\n",
    "    # Remember that the MSE is with respect to quantities scaled 0-1, so 0.01 is 1% error. \n",
    "    fig, axes = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(3.25, 2.25)\n",
    "    axes = axes.flatten()\n",
    "    train_line_args = dict(label=\"Training\", color=\"xkcd:navy\", lw=2.5, ls=\"-\")\n",
    "    test_line_args = dict(label=\"Validation\", color=\"xkcd:aqua\", ls=\"--\", lw=2.5)\n",
    "    epochs = np.arange(1, len(r2_history)+1)\n",
    "    axes[0].plot(epochs, hist.history['loss'], **train_line_args)\n",
    "    axes[0].plot(epochs, hist.history['val_loss'], **test_line_args)\n",
    "    axes[1].plot(epochs, r2_history,  **train_line_args)\n",
    "    axes[1].plot(epochs, val_r2_history, **test_line_args)\n",
    "    \n",
    "    # Annotate max R2\n",
    "    max_r2 = max(r2_history[-1], val_r2_history[-1])\n",
    "    min_r2 = min(r2_history[-1], val_r2_history[-1])\n",
    "    axes[1].annotate(\"{:.2f}\".format(max_r2), xy=(epochs[-1], min_r2*0.95), ha=\"right\", va=\"top\", size=7)\n",
    "    \n",
    "    axes[0].set_ylabel('Loss (MSE)')\n",
    "    axes[0].set_xlabel('No. epoch')\n",
    "    axes[0].legend(loc=\"upper right\", fontsize=7)\n",
    "    axes[1].legend(loc=\"lower right\", fontsize=7)\n",
    "    axes[1].set(ylabel=r\"$R^2$ score\", xlabel=\"No. epoch\")\n",
    "    axes[0].set_yscale(\"log\")\n",
    "    # Add minor ticks. minor_thresholds=(subset, all): subset is number of axis range decades below\n",
    "    # which only a subset of ticks is shown, all the threshold below which all minor ticks are labeled, \n",
    "    # and if decades > subset, no minor ticks are shown. \n",
    "    axes[0].yaxis.set_minor_formatter(mpl.ticker.LogFormatterSciNotation(minor_thresholds=(2, 1)))\n",
    "    axes[0].tick_params(which=\"minor\", axis=\"y\", labelsize=6)\n",
    "    axes[1].axhline(1.0, ls=\":\", color=\"grey\")\n",
    "    fig.tight_layout()\n",
    "    return fig, axes, r2_history, val_r2_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Seeding Python, numpy, tensorflow all necessary for reproducibility. \n",
    "%env PYTHONHASHSEED 0\n",
    "np.random.seed(3245134)\n",
    "python_random.seed(345345)\n",
    "\n",
    "# Tensorflow seed 2413598 gives similar latent space orientation\n",
    "tf_seed_regul = 2143598\n",
    "tf.random.set_seed(tf_seed_regul)\n",
    "\n",
    "# Creating the regularized autoencoder\n",
    "autoenc_regul = Autoencoder_regul(5, 2)\n",
    "# Nope: with regularization the latent space is a lot worse, everything on top of each other. \n",
    "autoenc_regul.compile(optimizer='adam', loss=ks.losses.MeanSquaredError())  #  gives poor results. \n",
    "history_regul = autoenc_regul.fit(df_train_norm.values.astype(np.float32), df_train_norm.values.astype(np.float32),                 \n",
    "            epochs=30,\n",
    "            shuffle=True,\n",
    "            validation_data=(df_test_norm.values, df_test_norm.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes, _, _ = plot_training_history(history_regul, df_train_norm, df_test_norm)\n",
    "axes[1].set_ylim(0, 1.1)\n",
    "fig.set_size_inches(3.25, 2.)\n",
    "# fig.savefig(os.path.join(main_dir_path, \"figures\", \"latentspaces\", \n",
    "#    \"autoencoder_training_history_integrals_regul.pdf\"), transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals_train_encoded_reg = autoenc_regul.encoder(df_train_norm.values.astype(np.float32)).numpy()\n",
    "df_train_encoded_reg = pd.DataFrame(vals_train_encoded_reg, index=df_train_norm.index, \n",
    "                                columns=pd.Index([\"LS1\", \"LS2\"], name=\"Latent variable\"), dtype=np.float32)\n",
    "\n",
    "\n",
    "vals_train_recon_reg = autoenc_regul.decoder(df_train_encoded_reg.values.astype(np.float32)).numpy()\n",
    "df_train_recon_reg = pd.DataFrame(vals_train_recon_reg, index=df_train_norm.index, columns=df_train_norm.columns)\n",
    "\n",
    "vals_test_encoded_reg = autoenc_regul.encoder(df_test_norm.values.astype(np.float32)).numpy()\n",
    "df_test_encoded_reg = pd.DataFrame(vals_test_encoded_reg, index=df_test_norm.index, \n",
    "                                columns=pd.Index([\"LS1\", \"LS2\"], name=\"Latent variable\"), dtype=np.float32)\n",
    "\n",
    "\n",
    "vals_test_recon_reg = autoenc_regul.decoder(df_test_encoded_reg.values.astype(np.float32)).numpy()\n",
    "df_test_recon_reg = pd.DataFrame(vals_test_recon_reg, index=df_test_norm.index, columns=df_test_norm.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the latent space for one dataset\n",
    "g = sns.relplot(data=df_train_encoded_reg.sort_index(level=\"Time\").reset_index(), \n",
    "            x=\"LS1\", y=\"LS2\", hue=\"Peptide\", size=\"Concentration\",\n",
    "            kind=\"line\", sort=False, style='TCellNumber', col=\"Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize reconstructions\n",
    "To see how well this autoencoder fares compared to the decoders we have trained for the paper. As it turns out, this autoencoder has basically the same reconstruction artifacts that we would get with simple reconstruction by linear regression. It does not beat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare data and reconstruction\n",
    "# Plot the latent space for one dataset\n",
    "df_train_both = pd.concat({\"Real\":df_train_norm, \"Recon\":df_train_recon_reg}, names=[\"Source\"], axis=0)\n",
    "sns.relplot(data=df_train_both.stack().xs(\"100k\", level=\"TCellNumber\").sort_index(level=\"Time\").to_frame().reset_index(), \n",
    "            x=\"Time\", y=0, hue=\"Peptide\", size=\"Concentration\",\n",
    "            kind=\"line\", style='Source', row=\"Data\", col=\"Cytokine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New antigens\n",
    "Where do they go with each autoencoder above? Do they line up properly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_peps = [\"A2\", \"Y3\"]\n",
    "df_allpeps_norm = (df.xs(\"integral\", level=\"Feature\", axis=1).copy() - df_min) / (df_max - df_min)\n",
    "df_allpeps_norm[\"Peptide set\"] = \"New\"\n",
    "# There are a couple peptide we don't use in general\n",
    "df_allpeps_norm = df_allpeps_norm.drop([\"Q7\", \"A8\"], level=\"Peptide\", axis=0)\n",
    "df_allpeps_norm.loc[df_allpeps_norm.index.isin(training_peps, level=\"Peptide\"), \"Peptide set\"] = \"Train\"\n",
    "df_allpeps_norm = df_allpeps_norm.set_index([\"Peptide set\"], append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_allpeps_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Also with the regularized autoencoder\n",
    "vals_allpeps_encoded_reg = autoenc_regul.encoder(df_allpeps_norm.values.astype(np.float32)).numpy()\n",
    "df_allpeps_encoded_reg = pd.DataFrame(vals_allpeps_encoded_reg, index=df_allpeps_norm.index, \n",
    "                                columns=pd.Index([\"LS1\", \"LS2\"], name=\"Latent variable\"), dtype=np.float32)\n",
    "\n",
    "vals_allpeps_recon_reg = autoenc_regul.decoder(df_allpeps_encoded_reg.values.astype(np.float32)).numpy()\n",
    "df_allpeps_recon_reg = pd.DataFrame(vals_allpeps_recon_reg, index=df_allpeps_norm.index, columns=df_allpeps_norm.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_space(df_ls, allpeps, x=\"LS1\", y=\"LS2\", \n",
    "                      dset_choice=\"PeptideComparison_4\", tcn_choice=\"100k\"):\n",
    "    default_colors = sns.color_palette()\n",
    "    palette = {allpeps[i]:default_colors[i] for i in range(len(allpeps))}\n",
    "    allpeps_order = [\"N4\", \"A2\", \"Y3\", \"Q4\", \"T4\", \"V4\", \"G4\", \"E1\"]\n",
    "    size_order = [\"1uM\", \"100nM\", \"10nM\", \"1nM\"]\n",
    "    \n",
    "    # Choose one training dataset, the one with most trajectories ideally\n",
    "    df_plot = df_ls.xs(dset_choice, level=\"Data\", drop_level=False)\n",
    "    # Add E1, G4 from all datasets\n",
    "    df_plot = df_plot.append(df_ls.loc[df_ls.index.isin([\"G4\", \"E1\"], level=\"Peptide\")])\n",
    "    df_plot = df_plot.droplevel(\"Data\")\n",
    "    chosen_lw = 1.\n",
    "    g = sns.relplot(data=(df_plot.sort_index(level=\"Time\").xs(tcn_choice, level=\"TCellNumber\").reset_index()), \n",
    "                x=x, y=y, sort=False, kind=\"line\", height=1.75, legend=True,\n",
    "                hue=\"Peptide\", hue_order=allpeps_order, palette=palette,\n",
    "                size=\"Concentration\", size_order=size_order,\n",
    "                style='Peptide set', style_order=[\"Train\", \"New\"])\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data with special line style for new antigens\n",
    "# Prepare colors\n",
    "chosen_dset = \"PeptideComparison_4\"\n",
    "g = plot_latent_space(df_allpeps_encoded_reg, training_peps+new_peps, \n",
    "                      dset_choice=chosen_dset, tcn_choice=\"100k\")\n",
    "g.fig.axes[0].set_xticks([])\n",
    "g.fig.axes[0].set_yticks([])\n",
    "g.fig.axes[0].set_xlabel(r\"LS${}_1$ (a.u.)\", labelpad=3.)\n",
    "g.fig.axes[0].set_ylabel(r\"LS${}_2$ (a.u.)\", labelpad=3.)\n",
    "g.fig.axes[0].set_aspect(\"equal\")\n",
    "g.legend.set_visible(False)\n",
    "handles, labels = g.fig.axes[0].get_legend_handles_labels()\n",
    "fig.set_size_inches(1.75*2, 1.75)\n",
    "g.fig.axes[0].legend(handles, labels, fontsize=6, ncol=2, loc=\"upper left\", bbox_to_anchor=(1, 1), \n",
    "                    frameon=False, columnspacing=1.)\n",
    "g.fig.tight_layout()\n",
    "#g.fig.savefig(\"figures/autoencoder_regularized_latent_space_{}.pdf\".format(dset_choice), \n",
    "#              transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA latent space\n",
    "This is another way to get a latent space. Does it look like the autoencoder space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also add PCA\n",
    "pcmodel = PCA(n_components=2)\n",
    "pcmodel_full = PCA(n_components=5)\n",
    "pcmodel.fit(df_train_norm)\n",
    "pcmodel_full.fit(df_train_norm)\n",
    "df_train_pca = pd.DataFrame(pcmodel.transform(df_train_norm), index=df_train_norm.index, \n",
    "                              columns=pd.Index([\"PC1\", \"PC2\"], name=\"Principal component\"))\n",
    "df_test_pca = pd.DataFrame(pcmodel.transform(df_test_norm), index=df_test_norm.index, \n",
    "                              columns=pd.Index([\"PC1\", \"PC2\"], name=\"Principal component\"))\n",
    "df_allpeps_pca = pd.DataFrame(pcmodel.transform(df_allpeps_norm), index=df_allpeps_norm.index, \n",
    "                              columns=pd.Index([\"PC1\", \"PC2\"], name=\"Principal component\"))\n",
    "print(pcmodel.explained_variance_ratio_)\n",
    "print(pcmodel_full.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data with special line style for new antigens\n",
    "# Prepare colors\n",
    "g = plot_latent_space(df_allpeps_pca, training_peps+new_peps, x=\"PC1\", y=\"PC2\",\n",
    "                      dset_choice=\"PeptideComparison_4\", tcn_choice=\"100k\")\n",
    "g.fig.axes[0].set_xticks([])\n",
    "g.fig.axes[0].set_yticks([])\n",
    "g.fig.axes[0].set_xlabel(\"PC 1 (a.u.)\", labelpad=3.)\n",
    "g.fig.axes[0].set_ylabel(\"PC 2 (a.u.)\", labelpad=3.)\n",
    "#g.fig.savefig(os.path.join(main_dir_path, \"figures\", \"latentspaces\", \n",
    "#        \"pca_latent_space_{}.pdf\".format(chosen_dset)), \n",
    "#        transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual information\n",
    "How does the autoencoder's latent space compare to our latent space and to PCA? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary concatenating the autoencoder and the classifier's latent spaces\n",
    "def create_dict_ls(dfs_dict, dset_choice, tcn_choice):\n",
    "    dict_ls = {}\n",
    "    for k, d in dfs_dict.items():\n",
    "        dat = d.loc[d.index.isin(dset_choice, level=\"Data\")]\n",
    "        if \"E1\" not in dat.index.get_level_values(\"Peptide\").unique():\n",
    "            dat = dat.append(d.xs(\"E1\", level=\"Peptide\", drop_level=False)).sort_index()\n",
    "        if tcn_choice not in [\"all\", None]:\n",
    "            dat = dat.xs(tcn_choice, level=\"TCellNumber\")\n",
    "        dict_ls[k] = dat\n",
    "    return dict_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projection matrix\n",
    "projmat = np.load(os.path.join(main_dir_path, \"data\", \"trained-networks\", \n",
    "                              \"mlp_input_weights-thomasRecommendedTraining.npy\"))\n",
    "\n",
    "df_allpeps_proj = df_allpeps_norm.dot(projmat)\n",
    "df_allpeps_proj.columns = pd.Index([\"LS1\", \"LS2\"], name='Variable')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "sns.relplot(data=df_allpeps_proj.reset_index(), x=\"LS1\", y='LS2', hue=\"Peptide\", kind=\"line\", sort=False,\n",
    "            style=\"TCellNumber\", col=\"Data\", size=\"Concentration\", col_wrap=5, hue_order=training_peps+new_peps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute MI over time windows\n",
    "dict_all_dfs_ls = {\n",
    "    #\"Autoencoder\": df_allpeps_encoded, \n",
    "    \"Autoencoder\": df_allpeps_encoded_reg, \n",
    "    \"Ag. classifier\": df_allpeps_proj, \n",
    "    \"PCA\": df_allpeps_pca\n",
    "}\n",
    "# Combine all training datasets to have better statistics. \n",
    "# Use only 100k T cells otherwise too much overlap. \n",
    "dsets_chosen = train_sets\n",
    "tcn_chosen = \"100k\"\n",
    "dict_latent_spaces = create_dict_ls(dict_all_dfs_ls, dsets_chosen, tcn_chosen)\n",
    "\n",
    "\n",
    "df_mi_time_encoded, max_mi_encoded = compute_mi_timecourse(\n",
    "    dict_latent_spaces, q=\"Peptide\", overlap=False, window=3, knn=3*3, speed=\"fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "#models_order = [\"5 cytokine conc.\", \"Ag. classifier\", \"Autoencoder\", \"PCA\"]\n",
    "models_order = [\"Ag. classifier\", \"Autoencoder\", \"PCA\"]\n",
    "g = sns.relplot(data=df_mi_time_encoded.reset_index().melt(id_vars=[\"Time\"], value_name=\"MI\", var_name=\"LS model\"), \n",
    "            x=\"Time\", y=\"MI\", hue=\"LS model\", kind=\"line\", height=3., palette=different_ls_colors, style=\"LS model\", lw=3., \n",
    "               hue_order=models_order, style_order=models_order)\n",
    "# Improve seaborn plot\n",
    "g.fig.axes[0].set_ylabel(\"MI (bits)\")\n",
    "g.fig.axes[0].set_xlabel(\"Time (h)\")\n",
    "g.fig.set_size_inches(2.25, 1.75)\n",
    "#g.fig.subplots_adjust(right=3/3.25)\n",
    "g.legend.loc = 1  # if required you can set the loc\n",
    "g.legend.set_bbox_to_anchor([0.95, 0.45])  # coordinates of lower left of bounding box\n",
    "g.fig.tight_layout()\n",
    "g.fig.tight_layout()\n",
    "for legobj in g.legend.legendHandles:\n",
    "    legobj.set_linewidth(2.0)\n",
    "g.fig.tight_layout()\n",
    "#g.fig.savefig(os.path.join(main_dir_path, \"figures\", \"latentspaces\", \n",
    "#        \"mi_vs_time_different_latent_spaces_train.pdf\"), \n",
    "#              transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export results\n",
    "This export was used to plot the network diagrams with proper weights between nodes. This plotting code is not included in the package, so there is no use to exporting the results below. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Export the results\n",
    "df_pca_components = pd.DataFrame(pcmodel_full.components_, \n",
    "                                 index=pd.RangeIndex(pcmodel_full.components_.shape[0], name=\"Component\").astype(int), \n",
    "                                columns=pd.Index(df_train.columns.values, name=\"Cytokine\"))\n",
    "df_pca_variance = pd.Series(pcmodel_full.explained_variance_, index=df_pca_components.index, \n",
    "                            name=\"explained_variance\").to_frame()\n",
    "df_pca_variance[\"explained_variance_ratio\"] = pcmodel_full.explained_variance_ratio_\n",
    "\n",
    "pcmodel_df = pd.concat({\"principal_vectors\": df_pca_components,   # each row is a component vector\n",
    "                \"variance\": df_pca_variance},   # Add as a column\n",
    "                names=[\"Attribute\"], axis=1)\n",
    "pcmodel_df.to_hdf(\"results/pca_integrals_components_variances.hdf\", key=\"pca\", mode=\"w\")\n",
    "\n",
    "print(pcmodel_df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Our customized Autoencoder classes can be saved and loaded in JSON format. \n",
    "# Feel free to reuse this code in utils/autoencoders.py\n",
    "save_autoencoder(autoenc_regul, 'autoencoder_regul_seed{}'.format(tf_seed_regul), folder=\"results/\", overwrite=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Try loading back the autoencoders\n",
    "load_autoencoder('autoencoder_regul_seed{}'.format(tf_seed_regul), folder=\"results/\")\n",
    "reloaded_auto = load_autoencoder('autoencoder_regul_seed{}'.format(tf_seed_regul), folder=\"results/\")\n",
    "for i, w in enumerate(reloaded_auto.weights):\n",
    "    assert np.all(w == autoenc_regul.weights[i]), \"Problem reloading the regularized integrals autoencoder\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train classifiers on top of the different latent spaces\n",
    "and compare to the original classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_peps_cytos_concs():\n",
    "    # Keep the train_peptides order as in the original code. E1, weakest, is first. \n",
    "    train_peptides = [\"N4\", \"Q4\", \"T4\", \"V4\", \"G4\", \"E1\"][::-1]\n",
    "    keep_cytokines = [\"IFNg\", \"IL-17A\", \"IL-2\", \"IL-6\", \"TNFa\"]\n",
    "    keep_conc = [\"1uM\", \"100nM\", \"10nM\", \"1nM\"]\n",
    "    keep_cytokines.sort()\n",
    "    return train_peptides, keep_cytokines, keep_conc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_peptides, keep_cytokines, keep_conc = init_peps_cytos_concs()\n",
    "peptide_dict = {k:v for v, k in enumerate(train_peptides) \n",
    "                         if k in df_allpeps_norm.index.get_level_values(\"Peptide\").unique()}\n",
    "#Extract times and set classes\n",
    "targets_enc = df_train_encoded_reg.index.get_level_values(\"Peptide\").map(peptide_dict).values.astype(int)\n",
    "targets_pca = df_train_pca.index.get_level_values(\"Peptide\").map(peptide_dict).values.astype(int)\n",
    "\n",
    "pca_classif = RidgeClassifier(alpha=1e-1, fit_intercept=True, max_iter=5000, \n",
    "                              solver=\"auto\", random_state=43251)\n",
    "auto_classif = RidgeClassifier(alpha=1e-1, fit_intercept=True, max_iter=5000, \n",
    "                               solver=\"auto\", random_state=249874198)\n",
    "\n",
    "pca_nn = MLPClassifier(activation=\"tanh\",hidden_layer_sizes=(),max_iter=5000,\n",
    "                    solver=\"adam\",random_state=1345209,learning_rate=\"adaptive\",alpha=0.01)\n",
    "auto_nn = MLPClassifier(activation=\"tanh\",hidden_layer_sizes=(),max_iter=5000,\n",
    "                    solver=\"adam\",random_state=9247148,learning_rate=\"adaptive\",alpha=0.01)\n",
    "\n",
    "# Fit\n",
    "#pca_classif.fit(df_train_pca, targets_pca)\n",
    "#auto_classif.fit(df_train_encoded, targets_enc)\n",
    "# Fit with cross-validation to have a reliable score estimate\n",
    "crossval_args = dict(cv=5, return_train_score=True, return_estimator=True)\n",
    "auto_scores = cross_validate(auto_classif, df_train_encoded_reg.values, targets_enc, **crossval_args)\n",
    "pca_scores = cross_validate(pca_classif, df_train_pca.values, targets_pca, **crossval_args)\n",
    "\n",
    "print(\"Starting training of neural networks...\")\n",
    "\n",
    "auto_scores_nn = cross_validate(auto_nn, df_train_encoded_reg.values, targets_enc, **crossval_args)\n",
    "print(\"Finished network for autoencoder; starting network for PCA...\")\n",
    "pca_scores_nn = cross_validate(pca_nn, df_train_pca.values, targets_pca, **crossval_args)\n",
    "\n",
    "# Rename test_score to cv_score \n",
    "auto_scores[\"cv_score\"] = auto_scores[\"test_score\"]\n",
    "pca_scores[\"cv_score\"] = pca_scores[\"test_score\"]\n",
    "auto_scores_nn[\"cv_score\"] = auto_scores_nn[\"test_score\"]\n",
    "pca_scores_nn[\"cv_score\"] = pca_scores_nn[\"test_score\"]\n",
    "\n",
    "print(auto_scores[\"train_score\"].mean(), \"pm\", auto_scores[\"train_score\"].std())\n",
    "print(pca_scores[\"train_score\"].mean(), \"pm\", pca_scores[\"train_score\"].std())\n",
    "print(auto_scores_nn[\"train_score\"].mean(), \"pm\", auto_scores_nn[\"train_score\"].std())\n",
    "print(pca_scores_nn[\"train_score\"].mean(), \"pm\", pca_scores_nn[\"train_score\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation with test data\n",
    "targets_test_enc = df_test_encoded_reg.index.get_level_values(\"Peptide\").map(peptide_dict).values.astype(int)\n",
    "targets_test_pca = df_test_pca.index.get_level_values(\"Peptide\").map(peptide_dict).values.astype(int)\n",
    "\n",
    "auto_scores[\"test_score\"] = np.asarray([cl.score(df_test_encoded_reg.values, targets_test_enc) \n",
    "                                        for cl in auto_scores[\"estimator\"]])\n",
    "pca_scores[\"test_score\"] = np.asarray([cl.score(df_test_pca.values, targets_test_pca) \n",
    "                                        for cl in pca_scores[\"estimator\"]])\n",
    "auto_scores_nn[\"test_score\"] = np.asarray([cl.score(df_test_encoded_reg.values, targets_test_enc) \n",
    "                                        for cl in auto_scores_nn[\"estimator\"]])\n",
    "pca_scores_nn[\"test_score\"] = np.asarray([cl.score(df_test_pca.values, targets_test_pca) \n",
    "                                        for cl in pca_scores_nn[\"estimator\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import projection matrix of our latent space classifier\n",
    "# Generate its latent space on the train and test data here\n",
    "# and train new output layer classifiers on it. \n",
    "# The score may be slightly different from the original classifier\n",
    "# because we remove the offsets and tanh applied on the latent space by it. \n",
    "projmat = np.load(os.path.join(main_dir_path, \"data\", \"trained-networks\", \n",
    "                              \"mlp_input_weights-thomasRecommendedTraining.npy\"))\n",
    "df_proj_train = df_train_norm.dot(projmat)\n",
    "df_proj_test = df_test_norm.dot(projmat)\n",
    "\n",
    "targets_mlp = df_proj_train.index.get_level_values(\"Peptide\").map(peptide_dict).values.astype(int)\n",
    "\n",
    "classif_mlp = MLPClassifier(activation=\"tanh\",hidden_layer_sizes=(),max_iter=5000,\n",
    "                    solver=\"adam\",random_state=92448,learning_rate=\"adaptive\",alpha=0.01)\n",
    "print(\"Starting cross-validation of classifier trained on the original classifier's latent space\")\n",
    "classif_scores = cross_validate(classif_mlp, df_proj_train.values, targets_mlp, **crossval_args)\n",
    "print(classif_scores[\"train_score\"].mean(), \"pm\", classif_scores[\"train_score\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_test_mlp = df_proj_test.index.get_level_values(\"Peptide\").map(peptide_dict).values.astype(int)\n",
    "classif_scores[\"cv_score\"] = classif_scores[\"test_score\"]\n",
    "classif_scores[\"test_score\"] = np.asarray([cl.score(df_proj_test.values, targets_test_mlp) \n",
    "                                        for cl in classif_scores[\"estimator\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nomograph of train and test score for each kind of model\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(3)\n",
    "i = 0\n",
    "for mod, sc_dict in zip([\"Ag. classifier\", \"PCA\", \"Autoencoder\"], [classif_scores, pca_scores_nn, auto_scores_nn]):\n",
    "    clr = different_ls_colors[mod]\n",
    "    y = [sc_dict[\"train_score\"].mean(), sc_dict[\"cv_score\"].mean(), sc_dict[\"test_score\"].mean()]\n",
    "    yerr = [sc_dict[\"train_score\"].std(), sc_dict[\"cv_score\"].std(), sc_dict[\"test_score\"].std()]\n",
    "    ax.errorbar(x, np.asarray(y)*100, yerr=np.asarray(yerr)*100, ls=\"-\", marker=\"o\", ms=8, label=mod, \n",
    "               color=clr, mfc=clr, mec=clr)\n",
    "    i += 1\n",
    "ax.legend(title=\"Latent space\", fontsize=7, frameon=False, loc=\"lower left\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([\"Train\", \"Cross-validate\", \"Test\"])\n",
    "ax.set_ylabel(\"Accuracy for Ag. prediction (%)\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(os.path.join(main_dir_path, \"figures\", \"latentspaces\", \n",
    "       \"train_cross_test_scores_different_latent_spaces.pdf\"), bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both MI and accuracy on one plot\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "fig.set_size_inches(5.25, 1.75)\n",
    "\n",
    "lw_graph = 3.\n",
    "lw_leg = 2.\n",
    "handlelength=3.\n",
    "\n",
    "# MI plot\n",
    "models_order = [\"Ag. classifier\", \"Autoencoder\", \"PCA\"]\n",
    "lstyles = {\"Ag. classifier\":\"-\", \"Autoencoder\":\"--\", \"PCA\":\":\"}\n",
    "ax = axes[0]\n",
    "times = sorted(df_mi_time_encoded.index.get_level_values(\"Time\").unique())\n",
    "for m in models_order:\n",
    "    y = df_mi_time_encoded[m].values\n",
    "    ax.plot(times, y, color=different_ls_colors[m], ls=lstyles[m], lw=lw_graph, label=m)\n",
    "ax.set_ylabel(\"MI (bits)\")\n",
    "ax.set_xlabel(\"Time (h)\")\n",
    "leg = ax.legend(title=\"Latent Space\", handlelength=handlelength, fontsize=8, frameon=False)\n",
    "for legobj in leg.legendHandles:\n",
    "    legobj.set_linewidth(lw_leg)\n",
    "\n",
    "# Score plot\n",
    "ax = axes[1]\n",
    "x = np.arange(3)\n",
    "i = 0\n",
    "for mod, sc_dict in zip([\"Ag. classifier\", \"PCA\", \"Autoencoder\"], [classif_scores, pca_scores_nn, auto_scores_nn]):\n",
    "    clr = different_ls_colors[mod]\n",
    "    y = [sc_dict[\"train_score\"].mean(), sc_dict[\"cv_score\"].mean(), sc_dict[\"test_score\"].mean()]\n",
    "    yerr = [sc_dict[\"train_score\"].std(), sc_dict[\"cv_score\"].std(), sc_dict[\"test_score\"].std()]\n",
    "    ax.errorbar(x, np.asarray(y)*100, yerr=np.asarray(yerr)*100, marker=\"o\", ms=8, label=mod, \n",
    "               color=clr, mfc=clr, mec=clr, lw=lw_graph, ls=lstyles[mod])\n",
    "    i += 1\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([\"Train\", \"Cross-validate\", \"Test\"])\n",
    "ax.set_ylabel(\"Accuracy for quality (%)\")\n",
    "fig.tight_layout(w_pad=6.)\n",
    "#fig.savefig(os.path.join(main_dir_path, \"figures\", \"latentspaces\", \n",
    "#        \"mi_accuracies_different_latent_spaces_train.pdf\"), \n",
    "#              transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the three latent spaces on the same figure, let the legend run over three rows\n",
    "def three_ls_plots(df_ag, df_au, df_pc, allpeps):\n",
    "    default_colors = sns.color_palette()\n",
    "    palette = {allpeps[i]:default_colors[i] for i in range(len(allpeps))}\n",
    "    allpeps_order = [\"N4\", \"A2\", \"Y3\", \"Q4\", \"T4\", \"V4\", \"G4\", \"E1\"]\n",
    "    size_order = [\"1uM\", \"100nM\", \"10nM\", \"1nM\"]\n",
    "    dset_choice = [\"PeptideComparison_4\"]\n",
    "    df_plot_combined = pd.concat({\"Ag. classifier\":df_ag, \n",
    "                    \"Autoencoder\":df_au, \n",
    "                    \"PCA\":df_pc.rename({\"PC1\":\"LS1\", \"PC2\":\"LS2\"}, level=\"Principal component\", axis=1)}, \n",
    "                    names=[\"Latent space\"], axis=0)\n",
    "    df_plot = df_plot_combined.loc[(slice(None), dset_choice),].xs(\"100k\", level=\"TCellNumber\")\n",
    "    df_plot = df_plot.append(df_plot_combined.loc[df_plot_combined.index.isin([\"G4\", \"E1\"], level=\"Peptide\")])\n",
    "    df_plot.droplevel(\"Data\")\n",
    "    chosen_lw = 1.0\n",
    "    \n",
    "    g = sns.relplot(data=df_plot.sort_index(level=\"Time\").reset_index(), \n",
    "                x=\"LS1\", y=\"LS2\", sort=False, kind=\"line\", height=1.75, legend=True,\n",
    "                hue=\"Peptide\", hue_order=allpeps_order, palette=palette,\n",
    "                size=\"Concentration\", size_order=size_order, row=\"Latent space\",\n",
    "                style='Peptide set', style_order=[\"Train\", \"New\"],\n",
    "                facet_kws={'sharey': False, 'sharex': False}, \n",
    "                row_order=[\"PCA\", \"Autoencoder\", \"Ag. classifier\"])\n",
    "    for ax in g.axes.flatten():\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_title(\"\")\n",
    "    g.fig.axes[0].set_ylabel(r\"PC$_2$\")\n",
    "    g.fig.axes[0].set_xlabel(r\"PC$_1$\", labelpad=2.5)\n",
    "    g.fig.axes[1].set_xlabel(r\"LS$_1$\", labelpad=2.5)\n",
    "    g.fig.axes[1].set_ylabel(r\"LS$_2$\")\n",
    "    g.fig.axes[2].set_xlabel(r\"LS$_1$\", labelpad=2.5)\n",
    "    g.fig.axes[2].set_ylabel(r\"LS$_2$\")\n",
    "    \n",
    "    g.fig.set_size_inches(5.25/2, 1.75*3)\n",
    "    g.fig.tight_layout(h_pad=4.)\n",
    "    g.fig.subplots_adjust(right=0.6)\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_proj_allpeps = df_allpeps_norm.dot(projmat)\n",
    "df_proj_allpeps.columns = pd.Index([\"LS1\", \"LS2\"], name=\"Variable\")\n",
    "g = three_ls_plots(df_proj_allpeps, df_allpeps_encoded_reg, df_allpeps_pca, training_peps+new_peps)\n",
    "#g.fig.savefig(os.path.join(main_dir_path, \"figures\", \"latentspaces\", \"latent_spaces_three_models.pdf\"), \n",
    "#   transparent=True, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credits\n",
    "Author of this script: frbourassa\n",
    "\n",
    "Keras-derived Autoencoder classes inspired by the official documentation and written by frbourassa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
