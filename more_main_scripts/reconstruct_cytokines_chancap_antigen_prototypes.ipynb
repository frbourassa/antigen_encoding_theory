{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruct cytokine trajectories for the theoretical antigen classes\n",
    "To run this notebook, you need:\n",
    "- To have run the notebook ``theoretical_antigen_classes_from_capacity_HighMI_3.ipynb`` (and hence its dependencies) and saved its results in ``results/capacity/``. \n",
    "- To have run the notebook `generate_synthetic_data.ipynb` and saved its outputs in the folder `results/reconstruction`, by default training of the decoder on a selection of 9 datasets (result files `quadratic_tanh_pipeline_selectdata.pkl` and `\"tanh_norm_factors_integrals_selectdata.hdf\"`)\n",
    "- Dataframe of model parameters ``results/fits/df_params_Sigmoid_freealpha20_reg04_selectdata.hdf``  fitted on an ensemble of 14 datasets, an output of the notebook ``latentspace_models.ipynb`` with the force model with matching. \n",
    "- Raw cytokine time series in `data/final/`, in particular the corrected dataset `\"cytokineConcentrationPickleFile-20210619-HighMI_3-final-corrected.pkl\"` (corrected by the channel capacity notebook above), to recover the absolute scale of cytokine concentrations after reconstruction. \n",
    "- Table of OT-1 antigens' EC$_{50}$s, values from the literature and from our own measurements, in the JSON file `data/misc/potencies_df_2021.json`. Also, for plotting aesthetics, tick parameters saved in JSON files in this folder. \n",
    "\n",
    "The files mentioned above are available in the Zenodo repository archive. \n",
    "\n",
    "## Motivation\n",
    "We use the EC$_{50}$s of the six antigen prototypes found from the channel capacity analysis, which was based on the HighMI_3 data. This dataset, with its many technical replicates for each antigen, allowed us to perform an accurate estimation of the variance of cytokine responses to different antigens, as it appears in the model parameter distributions. Therefore, we can use the covariance matrices (parameters $a_0$, $\\tau_0$, $\\theta$) and the variance of other parameters ($v_{t1}$, $\\alpha$, $\\beta$) fitted on these parameter distributions, and interpolated at the antigen classes' EC$_{50}$s. \n",
    "\n",
    "However, to have an accurate picture of the typical cytokine trajectories corresponding to those six antigen classes, it is more appropriate to use the *average* value of latent space parameters (and hence the average latent space trajectories) estimated from multiple datasets. It is also more appropriate to use a decoder (i.e. cytokine reconstruction coefficients from latent space) optimized on multiple datasets, rather than one that we would optimize on the latent space trajectories of the dataset used for channel capacity. Indeed, while the latter experiment provides excellent estimates of the *variance* of parameters (and thus of the mutual information and channel capacity), it does not necessarily reflect the average cytokine responses seen in most other experiments. Moreover, estimating the variance of parameters from an aggregate of multiple experimental repeats would be incorrect, as this variance is enlarged by batch effects and variability in various preparation steps or calibration standards over two years of experimental repeats. \n",
    "\n",
    "Therefore, to generate cytokine trajectories as representative as possible of the six antigen classes we derived, we use the noise structure estimated from the channel capacity analysis, but the average parameter values and the reconstruction decoder optimized on an ensemble of datasets (the ones used to generate figure S12, which used KDEs to estimate parameter distributions). So, basically, we build multivariate gaussian distributions interpolated as a function of EC$_{50}$, with covariances estimated with the channel capacity dataset, but averages estimated from many datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import os, json\n",
    "import sys\n",
    "main_dir_path = os.path.abspath('../')\n",
    "if main_dir_path not in sys.path:\n",
    "    sys.path.insert(0, main_dir_path)\n",
    "\n",
    "from utils.statistics import build_symmetric\n",
    "from utils.distrib_interpolation import (eval_interpolated_means_covs, \n",
    "                        interpolate_params_vs_logec50, stats_per_levels)\n",
    "from utils.extra_pairplots import dual_pairplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#plt.rcParams[\"figure.figsize\"] = (2.25, 1.75)\n",
    "plt.rcParams[\"axes.labelsize\"] = 8.\n",
    "plt.rcParams[\"legend.fontsize\"] = 8.\n",
    "plt.rcParams[\"axes.labelpad\"] = 0.5\n",
    "plt.rcParams[\"xtick.labelsize\"] = 7.\n",
    "plt.rcParams[\"ytick.labelsize\"] = 7.\n",
    "plt.rcParams[\"legend.title_fontsize\"] = 8.\n",
    "plt.rcParams[\"axes.titlesize\"] = 8.\n",
    "plt.rcParams[\"font.size\"] = 8.\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "\n",
    "# Load uniform tick props across the whole figure 3\n",
    "with open(os.path.join(main_dir_path, \"data\", \"misc\", \"minor_ticks_props.json\"), \"r\") as hd:\n",
    "    props_minorticks = json.load(hd)\n",
    "with open(os.path.join(main_dir_path, \"data\", \"misc\", \"major_ticks_props.json\"), \"r\") as hd:\n",
    "    props_majorticks = json.load(hd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate distributions as a function of EC$_{50}$\n",
    "We already have the covariance matrices for $a_0$, $t_0$, $\\theta$ and variances of other parameters at the right EC$_{50}$s from HighMI_3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the antigen classes' EC50s\n",
    "file_name = \"chancap_theo_antigen_trajectories_highmi3.hdf\"\n",
    "df_agclass_ec50s = pd.read_hdf(os.path.join(main_dir_path, \"results\", \"capacity\", file_name), key=\"ec50s\")\n",
    "\n",
    "# Import covariance and variance from HighMI_3\n",
    "df_agclass_covparams = pd.read_hdf(os.path.join(main_dir_path, \"results\", \"capacity\", file_name), key=\"covparams\")\n",
    "df_agclass_varparams = pd.read_hdf(os.path.join(main_dir_path, \"results\", \"capacity\", file_name), key=\"varparams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate mean parameters as a function of EC$_{50}$\n",
    "\n",
    "For the averages, we need to import empirical parameter distributions (like we do in the ``generate_synthetic_data.ipynb`` notebook), fit the averages, and interpolate them as a function of EC$_{50}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add error bars; can't use relplot or catplot anymore, \n",
    "# they don't have the option to include known standard deviations\n",
    "def plot_params_vs_logec50(df_estim, df_estim_vari, ser_x, cols_plot=None, \n",
    "                        ser_interp=None, x_name=\"Peptide\", col_wrap=3):\n",
    "    \"\"\" Optional df_interp, which contains interpolating splines for each parameter\n",
    "    as a function of the x variable (log EC50, usually). \n",
    "    \"\"\"\n",
    "    if cols_plot is None:\n",
    "        nplots = len(df_estim.columns)\n",
    "        cols_plot = df_estim.columns\n",
    "    else:\n",
    "        nplots = len(cols_plot)\n",
    "    \n",
    "    nrows = nplots // col_wrap + min(1, nplots % col_wrap)  # Add 1 if there is a remainder. \n",
    "    ncols = min(nplots, col_wrap)\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows, ncols, sharex=True, sharey=False)\n",
    "    fig.set_size_inches(3*ncols, 2.5*nrows)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i in range(nplots):\n",
    "        estim = df_estim[cols_plot[i]]\n",
    "        stds = np.sqrt(df_estim_vari[cols_plot[i]])\n",
    "        x_labels = estim.index.get_level_values(x_name)\n",
    "        xpoints = ser_x.reindex(x_labels)  # assume ser_x has a single index level?\n",
    "        xmin, xmax = np.amin(xpoints), np.amax(xpoints)\n",
    "        axes[i].errorbar(xpoints, estim, yerr=stds, ls='none', marker=\"o\", ms=5, label=\"Fit\")\n",
    "        axes[i].set_ylabel(cols_plot[i])\n",
    "        xr = xmax - xmin\n",
    "        yr = np.amax(estim) - np.amin(estim)\n",
    "        for j in range(len(x_labels)):\n",
    "            axes[i].annotate(x_labels[j], xy=(xpoints[j]+0.01*xr, estim[j]+0.02*yr), fontsize=8)\n",
    "        if ser_interp is not None:\n",
    "            spl = ser_interp[cols_plot[i]]\n",
    "            xrange = np.linspace(xmin, xmax, 201)\n",
    "            axes[i].plot(xrange, spl(xrange), lw=1.5, label=\"Interpolation\")\n",
    "            axes[i].legend()\n",
    "    return [fig, axes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample_p = pd.read_hdf(os.path.join(main_dir_path, \"results\", \"fits\", \n",
    "                    \"df_params_Sigmoid_freealpha_reg04_selectdata.hdf\"), key=\"df_params\")\n",
    "\n",
    "# Remove different T cell numbers if necessary\n",
    "tcn_fit = \"100k\"\n",
    "try:\n",
    "    df_sample_p = df_sample_p.xs(tcn_fit, level=\"TCellNumber\", axis=0, drop_level=True)\n",
    "except KeyError: \n",
    "    print(\"TCellNumber was already sliced; skipping.\")\n",
    "\n",
    "# Drop A8 and Q7, which we don't use in general. \n",
    "df_sample_p = df_sample_p.drop([\"A8\", \"Q7\"], level=\"Peptide\")\n",
    "print(df_sample_p.index.get_level_values(\"Data\").unique())\n",
    "\n",
    "# Only keep G4 and E1 if theta < 0; we know that others are outliers,\n",
    "# artifacts due to insufficient regularization or background fluorescence\n",
    "df_sample_p = df_sample_p.loc[np.logical_not(df_sample_p.index.isin([\"E1\", \"G4\"], level=\"Peptide\")&(df_sample_p[\"theta\"] > 0.0))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit distributions on the imported parameter samples\n",
    "params_with_cov = [\"a0\", \"tau0\", \"theta\"]\n",
    "levels_group = [\"Peptide\"]\n",
    "\n",
    "# THIS IS WHERE THE GAUSSIANS ARE ACTUALLY FITTED. Only keep the means\n",
    "ret = stats_per_levels(df_sample_p, levels_groupby=levels_group, feats_keep=params_with_cov)\n",
    "df_p_means, df_p_means_estim_vari, _, _, _ = ret\n",
    "\n",
    "# We might have zero variance on E1/G4 which are at zero\n",
    "# This can cause bugs when interpolating with zero error bars, so set variance to very small value\n",
    "df_p_means_estim_vari = df_p_means_estim_vari.clip(lower=np.abs(df_p_means).min().min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import EC50s of peptides\n",
    "df_potencies = pd.read_json(os.path.join(main_dir_path, \"data\", \"misc\", \"potencies_df_2021.json\"))\n",
    "ser_log10ec50s = np.log10(df_potencies).mean(axis=1)\n",
    "ser_log10ec50s.index.name = \"Peptide\"\n",
    "# Only keep peptides for which we have parameter values\n",
    "ser_log10ec50s = ser_log10ec50s.loc[df_sample_p.index.get_level_values(\"Peptide\").unique()]\n",
    "print(ser_log10ec50s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_splines_means = interpolate_params_vs_logec50(df_p_means, df_p_means_estim_vari, \n",
    "                                      ser_log10ec50s, x_name=\"Peptide\")\n",
    "\n",
    "# Plot the interpolation versus the data\n",
    "fig, axes = plot_params_vs_logec50(df_p_means, df_p_means_estim_vari, ser_log10ec50s, \n",
    "                             ser_interp=ser_splines_means, cols_plot=None, x_name=\"Peptide\", col_wrap=3)\n",
    "for ax in axes[-3:]:  # 3 is col_wrap\n",
    "    ax.set_xlabel(r\"$\\log_{10}{\\mathrm{EC}_{50}}$ [-]\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the means at the antigen classes' EC50s\n",
    "n_categories = len(df_agclass_ec50s.index.get_level_values(\"TheoreticalAntigen\").unique())\n",
    "interpolated_means = np.zeros([n_categories, 3])\n",
    "for i in range(n_categories):\n",
    "    interpolated_means[i, 0] = ser_splines_means[\"a0\"](df_agclass_ec50s[\"log10ec50\"].iloc[i])\n",
    "    interpolated_means[i, 1] = ser_splines_means[\"tau0\"](df_agclass_ec50s[\"log10ec50\"].iloc[i])\n",
    "    interpolated_means[i, 2] = ser_splines_means[\"theta\"](df_agclass_ec50s[\"log10ec50\"].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For other parameters (v1, alpha, beta), we use linear interpolation\n",
    "# since we do not bother to get error bars on the estimates on the means\n",
    "# and the behaviour of those parameters as a function of EC50s is not so\n",
    "# monotonic, we better avoid cubic splines like we used for a0, tau0, theta. \n",
    "def interpolate_nearest_peptides(logec50, pep_logec50s, ser_values):\n",
    "    \"\"\" Given an arbitrary log_10 EC_50, a list of peptide log_10 EC_50s, and a \n",
    "    value of the quantity to interpolate for each peptide label, find the two peptides\n",
    "    closest to the desired EC_50 and interpolate linearly between their values. \"\"\"\n",
    "    # Find the peptide below and the peptide above\n",
    "    sorted_ec50s = pep_logec50s.sort_values()\n",
    "    ec50_index_above = np.searchsorted(sorted_ec50s, logec50, side=\"left\")\n",
    "    try:\n",
    "        ec50_above = sorted_ec50s.iloc[ec50_index_above]\n",
    "    except IndexError:\n",
    "        raise ValueError(\"We are above the interpolation range\")\n",
    "    else:\n",
    "        pep_above = sorted_ec50s.index.to_series().iloc[ec50_index_above]\n",
    "    \n",
    "    try:\n",
    "        ec50_below = sorted_ec50s.iloc[ec50_index_above-1]\n",
    "    except IndexError:\n",
    "        raise ValueError(\"We are below the interpolation range\")\n",
    "    else:\n",
    "        pep_below = sorted_ec50s.index.to_series().iloc[ec50_index_above-1]\n",
    "    \n",
    "    # Find the parameter values below and above\n",
    "    try:\n",
    "        value_below = ser_values[pep_below]\n",
    "        value_above = ser_values[pep_above]\n",
    "    except KeyError as e:\n",
    "        print(\"Peptide {} not available; check consistency of EC50 and parameter tables.\")\n",
    "        raise e\n",
    "    \n",
    "    # Interpolate linearly\n",
    "    value_inter = (logec50 - ec50_below) / (ec50_above - ec50_below) * (value_above - value_below) + value_below\n",
    "    return value_inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser_means_pars = {\n",
    "    \"v1\": df_sample_p[\"v1\"].groupby(\"Peptide\").mean(), \n",
    "    \"alpha\": df_sample_p[\"alpha\"].groupby(\"Peptide\").mean(), \n",
    "    \"beta\": df_sample_p[\"beta\"].groupby(\"Peptide\").mean()\n",
    "}\n",
    "ideal_peptides_par_means = {\n",
    "    \"v1\": np.asarray(list(map(lambda x: interpolate_nearest_peptides(x, ser_log10ec50s, ser_means_pars[\"v1\"]), \n",
    "                             df_agclass_ec50s[\"log10ec50\"].values))), \n",
    "    \"alpha\": np.asarray(list(map(lambda x: interpolate_nearest_peptides(x, ser_log10ec50s, ser_means_pars[\"alpha\"]), \n",
    "                             df_agclass_ec50s[\"log10ec50\"].values))),\n",
    "    \"beta\": np.asarray(list(map(lambda x: interpolate_nearest_peptides(x, ser_log10ec50s, ser_means_pars[\"beta\"]), \n",
    "                             df_agclass_ec50s[\"log10ec50\"].values)))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the mean parameters evaluated so far (a0, tau0, theta; v1, alpha, beta)\n",
    "df_agclass_meanparams = pd.DataFrame(np.zeros([n_categories, 6]), \n",
    "                        index=df_agclass_ec50s.index,  \n",
    "                        columns=pd.Index([\"a0\", \"tau0\", \"theta\", \"v1\", \"alpha\", \"beta\"], \n",
    "                                            name=\"Parameter\"))\n",
    "df_agclass_meanparams.iloc[:, :3] = interpolated_means\n",
    "\n",
    "for i, pch in zip((3, 4, 5), (\"v1\", \"alpha\", \"beta\")):\n",
    "    df_agclass_meanparams.iloc[:, i] = ideal_peptides_par_means[pch]\n",
    "\n",
    "print(df_agclass_meanparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sigmoid model trajectories in latent space\n",
    "Generate the average trajectories from the mean parameters. Also generate a bunch of parameter values and associated latent space trajectories, using the variance and covariance of parameters we also have for each antigen class. \n",
    "\n",
    "This part has a tricky aspect: we fitted $LS_i(t')$, where $t' = t/\\tilde{t}$, $\\tilde{t} = 20 $ h (the time scale). Now, we want $ls_i(t) = \\frac{d N_i}{dt} = \\frac{d t'}{dt} \\frac{d N_i(t')}{d t'} = \\frac{1}{\\tilde{t}} n_i(t', a_0', \\ldots)$, where $ls_i(t', a_0', \\ldots)$ is the formal function $ls_i(t)$ called with $t'$ and parameters fitted for $LS_i(t')$, instead of $t$: same functional form, different scale of variables. We need to compensate this by dividing by $\\tilde{t}$, because when fitting $LS_i(t')$, we have the following things happening:\n",
    " - To preserve $\\alpha t = \\alpha' t'$ and $\\beta t = \\beta' t'$ in the exponentials with $t$ replaced by $t'$, $\\alpha' = \\tilde{t} \\alpha$\n",
    " - Because the magnitude of $LS_i$ is proportional to $a_0 / \\alpha$ or $v_i / \\alpha$, then $a_0' = \\tilde{t} a_0$, i.e. the fitted value for $a_0$ is too large in reality by a factor of $t_0$\n",
    " - Since the functional form of $ls_i$ has a magnitude proportional to $a_0$ only, calling that function with the fitted value of $a_0'$ would give a concentration too large by a factor $\\tilde{t}$ for being truly $\\frac{d N_i}{dt}$. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltspcyt.scripts.sigmoid_ballistic import sigmoid_conc_full_freealpha, ballistic_sigmoid_freealpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synth_sample_params(df_m, df_c, df_v, p_with, v2v1, rng=None, nsamp=100):\n",
    "    \"\"\" Function that generates nsamp parameter distribution samples for each\n",
    "    key in the index of df_m, which contains the average value of parameters for each key\n",
    "    (usually, the keys are the different peptides). df_c and df_v contain the covariance\n",
    "    matrices and variance of the parameters for each key. p_with specifies which\n",
    "    parameters are in the covariance matrix, and which are without covariance and  only\n",
    "    appear in the variances of df_v. v2v1 is the ratio of v2/v1 at late times. \n",
    "    rng is a random number generator (np.random.RandomGenerator). \n",
    "    \"\"\"\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng()\n",
    "    new_idx = pd.MultiIndex.from_product([df_m.index, range(nsamp)],\n",
    "                                            names=[df_m.index.name, \"Sample\"])\n",
    "    df_p = pd.DataFrame(np.zeros([len(df_m.index)*nsamp, len(df_m.columns)]),\n",
    "                index=new_idx, columns=df_m.columns)\n",
    "    # Treat parameters that require a covariance matrix\n",
    "    # Generate nsamp samples for each key in the index. \n",
    "    for key in df_m.index:\n",
    "        cov_mat = build_symmetric(df_c.loc[key])\n",
    "        mean_vec = df_m.loc[key, p_with].values\n",
    "        df_p.loc[key, p_with] = rng.multivariate_normal(mean_vec, cov_mat, nsamp)\n",
    "    \n",
    "    # Now treat parameters that have separate variance\n",
    "    other_p = list(df_m.columns)\n",
    "    for p in p_with:\n",
    "        other_p.remove(p)\n",
    "    for key in df_m.index:\n",
    "        varis = df_v.loc[key, other_p].values\n",
    "        mean_vec = df_m.loc[key, other_p].values\n",
    "        df_p.loc[key, other_p] = (mean_vec\n",
    "            + np.sqrt(varis).reshape(1, -1)\n",
    "                *rng.normal(size=[nsamp, len(other_p)]))\n",
    "\n",
    "    # At the end, clip parameter a0, tau0, v1, alpha, beta to be >= 0\n",
    "    # Clip alpha and beta to tscale/100 = 1/5\n",
    "    params_clips = {\"a0\":(0, np.inf), \"tau0\":(0, np.inf), \"t0\":(0, np.inf), \n",
    "                   \"v1\":(0, np.inf), \"alpha\":(0.2, np.inf), \"beta\":(0.2, np.inf)}\n",
    "    for p, bounds in params_clips.items():\n",
    "        try:\n",
    "            df_p[p].clip(*bounds, inplace=True)\n",
    "        except KeyError: continue\n",
    "    return df_p\n",
    "\n",
    "\n",
    "def compute_lsmodel_trajectories(df_p_samp, times, v2v1, tscale=20.0):\n",
    "    \"\"\" For each row giving LS model parameter values in df_p_samp,\n",
    "    compute latent space trajectories, to be reconstructed later.\n",
    "    Compute both n_i and N_i. \"\"\"\n",
    "    # Normalize time\n",
    "    taus = times / tscale\n",
    "    # Initialize DataFrame\n",
    "    idx = pd.MultiIndex.from_tuples([(*k, t) for t in range(len(times)) for k in df_p_samp.index],\n",
    "                                names=[*df_p_samp.index.names] + [\"Time\"])\n",
    "    cols = pd.Index([\"ls1\", \"ls2\", \"LS1\", \"LS2\"], name=\"Feature\")\n",
    "    df_traj = pd.DataFrame(np.zeros([len(df_p_samp.index)*len(times), len(cols)]),\n",
    "                            index=idx, columns=cols)\n",
    "    df_traj = df_traj.sort_index()\n",
    "    # Compute trajectories for each parameter set\n",
    "    # Make sure we have the right parameters.\n",
    "    assert np.all(np.asarray([\"a0\", \"tau0\", \"theta\", \"v1\", \"alpha\", \"beta\"]) == df_p_samp.columns.values)\n",
    "    for key in df_p_samp.index:\n",
    "        params = df_p_samp.loc[key].values\n",
    "        ls12 = sigmoid_conc_full_freealpha(taus, params, v2v1_ratio=v2v1)\n",
    "        # Normalize n1 and n2 by tscale, due to derivative wrt tau from N1, n2\n",
    "        df_traj.loc[key, \"ls1\":\"ls2\"] = ls12.T / tscale\n",
    "        LS12 = ballistic_sigmoid_freealpha(taus, *params, v2v1_ratio=v2v1)\n",
    "        df_traj.loc[key, \"LS1\":\"LS2\"] = LS12.T\n",
    "\n",
    "    return df_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample parameter tuples for each antigen class\n",
    "nsamples = 100\n",
    "# Pick typical v2/v1 for the datasets used\n",
    "v2v1_slope = pd.read_hdf(os.path.join(main_dir_path, \"results\", \"reconstruction\", \n",
    "                        \"ser_v2v1_synth_selectdata.hdf\"), key=\"ser_v2v1_synth\").mean()\n",
    "time_scale = 20.0\n",
    "rdgen = np.random.default_rng(seed=292192031)\n",
    "df_synth_p = generate_synth_sample_params(df_agclass_meanparams, df_agclass_covparams,\n",
    "                df_agclass_varparams, params_with_cov, v2v1=v2v1_slope, rng=rdgen, nsamp=nsamples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the empirical parameter to synthetic parameters samples\n",
    "idx = np.concatenate([np.arange(n) for n in df_sample_p.groupby(\"Peptide\").count().sort_index().values[:, 0]])\n",
    "df_both_p = df_sample_p.reset_index().set_index(\"Peptide\")\n",
    "df_both_p = df_both_p.rename_axis(index={\"Peptide\":\"TheoreticalAntigen\"})\n",
    "\n",
    "df_both_p[\"Sample\"] = idx\n",
    "df_both_p = df_both_p.set_index(\"Sample\", append=True)\n",
    "df_both_p = pd.concat({\"Data\":df_both_p, \"Synthetic\":df_synth_p}, names=[\"Source\"], axis=0)\n",
    "\n",
    "#pep_order = [\"N4\", \"Q4\", \"T4\", \"V4\", \"G4\", \"E1\", \"A2\", \"Y3\"]\n",
    "#fig, axes, legend2 = dual_pairplot(data=df_both_p.reset_index(), vari=list(df_synth_p.columns),\n",
    "#                dual_lvl=\"Source\", dual_labels=[\"Data\", \"Synthetic\"],\n",
    "#                dual_hues = [(0.5, 0.5, 0.5), plt.cm.viridis([206])[0]],\n",
    "#                hue=\"IdealPeptide\", hue_order=pep_order, s=12, alpha=0.7)\n",
    "#fig.set_size_inches(2*6, 2*6)\n",
    "#fig.tight_layout()\n",
    "#plt.show()\n",
    "#plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute latent space curves from the generated parameters. The function compute_lsmodel_trajectories\n",
    "# only computes ls_i and LS_i, not the tanh(LS_i), which will have to be added before reconstruction. \n",
    "tpoints = np.arange(0, 73, dtype=int)  # Use ints because nicer as index.\n",
    "df_latent_synth = compute_lsmodel_trajectories(df_synth_p, tpoints, v2v1_slope, tscale=time_scale)\n",
    "\n",
    "df_latent_synthmean = compute_lsmodel_trajectories(pd.concat({1:df_agclass_meanparams}, names=[\"Sample\"]), \n",
    "                                                   tpoints, v2v1_slope, tscale=time_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=df_latent_synthmean.stack(\"Feature\").reset_index(), \n",
    "            col=\"Feature\", x=\"Time\", y=0, hue=\"TheoreticalAntigen\", kind=\"line\", facet_kws=dict(sharey=False), \n",
    "           palette={i:col for i, col in enumerate(sns.color_palette(\"deep\"))}, height=2.5, lw=2.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.relplot(data=df_latent_synthmean.reset_index(), x=\"LS1\", y=\"LS2\", hue=\"TheoreticalAntigen\", kind=\"line\",\n",
    "           palette={i:col for i, col in enumerate(sns.color_palette(\"deep\"))}, height=2.5, sort=False)\n",
    "g.axes[0, 0].set_aspect(\"equal\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct cytokine trajectories\n",
    "Both for the average trajectory of each antigen category, and for all the sampled trajectories around them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ltspcyt.scripts.reconstruction import QuadraticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_from_lstraj(df_ls, pipeline, tanh_norm, cyorder):\n",
    "    \"\"\" Reconstruct cytokine time series from latent space trajectories.\n",
    "    Use a pre-trained pipeline (basically contains a linear regression matrix)\n",
    "    and pre-determined the normalization coefficients tanh_norm. \n",
    "    The reconstructions not scaled back to proper log10(pM) scale. \n",
    "    The lower LOD corresponds to zero in the reconstruction output.  \n",
    "    The scaling back and offsetting has to be done separately later. \n",
    "    \"\"\"\n",
    "    # Add features necessary to reconstruction\n",
    "    df_features = df_ls.copy()\n",
    "    df_features[\"tanh_LS1\"] = np.tanh(df_ls[\"LS1\"]/tanh_norm[\"LS1\"])\n",
    "    df_features[\"tanh_LS2\"] = np.tanh(df_ls[\"LS2\"]/tanh_norm[\"LS2\"])\n",
    "    df_features = df_features.drop([\"LS1\", \"LS2\"], axis=1)\n",
    "    # Make sure the order of features is correct\n",
    "    feat_order = [\"ls1\", \"ls2\", \"tanh_LS1\", \"tanh_LS2\"]\n",
    "    df_features = df_features.reindex(columns=pd.Index(feat_order, name=\"Feature\"))\n",
    "\n",
    "    # Reconstruct from features: one big dot product with recon. matrix\n",
    "    # recon_matrix has shape [n_cytos, n_features] so need to transpose\n",
    "    # and dot with recon_matrix to the right, so the dimensions left\n",
    "    # are [n_samples, n_cytos]\n",
    "    df_cytos = pd.DataFrame(pipeline.predict(df_features), index=df_features.index, \n",
    "                             columns=cyto_order)\n",
    "    df_cytos.columns = pd.Index(cyorder, name=\"Cytokine\")\n",
    "\n",
    "    # Clip\n",
    "    df_cytos = df_cytos.clip(0, np.inf)\n",
    "\n",
    "    # Rescale to proper log10 scale\n",
    "    #df_cytos = df_cytos * cyscales.reshape(1, -1)\n",
    "    return df_cytos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmaxfile = os.path.join(main_dir_path, \"data\", \"trained-networks\", \"min_max-thomasRecommendedTraining.hdf\")\n",
    "df_min = pd.read_hdf(minmaxfile, key=\"df_min\")\n",
    "df_max = pd.read_hdf(minmaxfile, key=\"df_max\")\n",
    "df_min, df_max = df_min.xs(\"integral\", level=\"Feature\"), df_max.xs(\"integral\", level=\"Feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import reconstruction objects\n",
    "model_type = \"mixed_quad\"\n",
    "recon_folder = os.path.join(main_dir_path, \"results\", \"reconstruction\")\n",
    "with open(os.path.join(recon_folder, \"quadratic_tanh_pipeline_selectdata.pkl\"), \"rb\") as hd:\n",
    "    pipe = pickle.load(hd)\n",
    "tanh_norm_factors = pd.read_hdf(os.path.join(recon_folder, \"tanh_norm_factors_integrals_selectdata.hdf\"), \n",
    "                                key=\"tanh_norm\")\n",
    "tanh_norm_factors = tanh_norm_factors.rename({\"Node 1\":\"LS1\", \"Node 2\":\"LS2\"}, axis=1)\n",
    "print(tanh_norm_factors)\n",
    "print(pipe[model_type].regressor_.Q)\n",
    "\n",
    "# Reconstruct sampled and mean latent space trajectories\n",
    "cyto_order = df_min.index.get_level_values(\"Cytokine\")\n",
    "print(cyto_order)\n",
    "df_recon_synth = reconstruct_from_lstraj(df_latent_synth, pipe, tanh_norm_factors, cyto_order)\n",
    "df_recon_synthmean = reconstruct_from_lstraj(df_latent_synthmean, pipe, tanh_norm_factors, cyto_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the scaled cytokine trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# It is nicer to average all sampled trajectories than to use the trajectory for average parameters. \n",
    "df_recon_synthmean2 = df_recon_synth.groupby([\"TheoreticalAntigen\", \"Time\"]).mean()\n",
    "sns.relplot(data=df_recon_synthmean2.stack(\"Cytokine\").reset_index(), x=\"Time\", y=0, \n",
    "    col=\"Cytokine\", hue=\"TheoreticalAntigen\", height=2.5, kind=\"line\", \n",
    "    palette=sns.color_palette(\"deep\", \n",
    "    n_colors=len(df_recon_synthmean.index.get_level_values(\"TheoreticalAntigen\").unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rescale and export time integrals of reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_back(df_cyt, dfmin, dfmax):\n",
    "    \"\"\" Take scaled cytokine data/reconstruction, and put it back in log_10 scale. \"\"\"\n",
    "    feat_keys = [\"integral\", \"concentration\", \"derivative\"]\n",
    "    df_scaled = df_cyt.copy()\n",
    "    for typ in feat_keys:\n",
    "        try:\n",
    "            df_scaled[typ] = df_cyt[typ] * (dfmax - dfmin)\n",
    "            if typ == \"integral\":\n",
    "                df_scaled[typ] = df_scaled[typ] + dfmin\n",
    "        except KeyError:\n",
    "            continue  # This feature isn't available in this df; fine\n",
    "        else:\n",
    "            print(\"Put scale back for feature\", typ)\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the lower LOD (lowest concentration in pM, which was set to zero by the log-transform)\n",
    "# Parse LOD files, take the geometric average lowest concentration detected\n",
    "# in the experiments used to estimate the average parameter values\n",
    "all_kept_lower_lods = {}\n",
    "lod_path = os.path.join(main_dir_path, \"data/\", \"LOD/\")\n",
    "for f in os.listdir(lod_path):\n",
    "    if not f.endswith(\".json\"): continue\n",
    "    try:\n",
    "        loddf = pd.read_json(os.path.join(lod_path, f))\n",
    "    except:\n",
    "        print(\"Could not load LOD file\" + f)\n",
    "        continue\n",
    "    key = \"-\".join(f.split(\"-\")[2:-1])\n",
    "    if key in df_sample_p.index.get_level_values(\"Data\").unique():\n",
    "        lower_lod = loddf[\"('Concentration', 'Lower')\"]\n",
    "        all_kept_lower_lods[key] = lower_lod\n",
    "all_kept_lower_lods = pd.concat(all_kept_lower_lods, axis=1, names=[\"Data\"]).T\n",
    "print(all_kept_lower_lods)\n",
    "pM_offsets = np.exp((np.log(all_kept_lower_lods).mean())) * 1000\n",
    "print(pM_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels_to_stack = list(df_recon_synth.index.names)\n",
    "levels_to_stack.remove(\"Time\")\n",
    "df_integrals = (df_recon_synth.copy().unstack(levels_to_stack).sort_index()\n",
    "                .cumsum(axis=0).stack(levels_to_stack).unstack(\"Time\").stack(\"Time\"))\n",
    "df_recon_combined = pd.concat({\"concentration\":df_recon_synth, \"integral\":df_integrals}, \n",
    "                             axis=1, names=[\"Feature\", \"Cytokine\"])\n",
    "df_recon_combined = scale_back(df_recon_combined, df_min, df_max)\n",
    "df_recon_combined[\"concentration\"] += np.log10(pM_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_recon_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add time integrals, save the results\n",
    "#df_recon_combined.to_hdf(\"output_recon/synthetic/antigen_prototypes_recon_conc_integrals_HighMI13_training-means.hdf\", key=\"df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proper plot of cytokine time series for antigen classes\n",
    "Average time series with shaded standard deviation at each time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use same color palette as in the main text\n",
    "\n",
    "all_theo_antigen_colors = sns.color_palette(\"deep\", 10)\n",
    "theoretical_antigen_colors = sns.color_palette(\"deep\", n_categories)\n",
    "# Make the second class have a fuchsia color similar to A2\n",
    "theoretical_antigen_colors = [all_theo_antigen_colors[0],all_theo_antigen_colors[6]]+all_theo_antigen_colors[1:5]\n",
    "theoretical_antigen_colors = [sns.set_hls_values(a, s=0.4, l=0.6) for a in theoretical_antigen_colors]\n",
    "theoretical_antigen_colors[-1] = (0, 0, 0, 1)  # Make the null peptide black. \n",
    "\n",
    "# Color for smaller, lighter sample trajectories. Not used here, using alphas instead. \n",
    "#colors_samples = [sns.set_hls_values(a, l=0.8) for a in theoretical_antigen_colors]\n",
    "#colors_samples[-1] = (0.5, 0.5, 0.5, 0.8)\n",
    "#colors_samples = theoretical_antigen_colors\n",
    "\n",
    "idealpeps = df_recon_synth.index.get_level_values(\"TheoreticalAntigen\").unique()\n",
    "colors_dict = {idealpeps[i]:theoretical_antigen_colors[i] for i in range(len(idealpeps))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logarithmic minor ticks (we plotted the real log so need to put log ticks manually)\n",
    "# Find the linear scale limiting ticks\n",
    "def compute_log_minor_ticks(loglims, stp=2, base=10.0):\n",
    "    smallest_major = int(np.floor(loglims[0]))\n",
    "    largest_major = int(np.ceil(loglims[1]))\n",
    "    n_decades = largest_major - smallest_major\n",
    "\n",
    "    # Generate linear ranges with the exponents found\n",
    "    tiles = []\n",
    "    for i in range(n_decades):\n",
    "        tiles.append(np.arange(stp*base**(smallest_major+i), \n",
    "                    base**(smallest_major+i+1), stp*base**(smallest_major+i)))\n",
    "    minorticks = np.concatenate(tiles, axis=0)\n",
    "    minorticks = np.log(minorticks) / np.log(base)\n",
    "    minorticks = minorticks[(minorticks > loglims[0]) * (minorticks < loglims[1])]\n",
    "    return minorticks\n",
    "\n",
    "# Formatting log(number) into \"number \\times 10^power\" string\n",
    "def format_scinotation(logf, n_decim=0):\n",
    "    \"\"\" Transform the log_10 of a number, given by logf, into a string\n",
    "    in scientific notation, a \\times 10^{power}, where a has n_decim decimals\"\"\"\n",
    "    pwr = int(np.floor(logf))\n",
    "    num = 10**(logf - pwr)\n",
    "    num = round(num, n_decim)\n",
    "    if n_decim == 0:\n",
    "        if num >= 10:\n",
    "            pwr += 1\n",
    "            num = num // 10\n",
    "        num = \"{\" + str(int(num)) + \"}\"\n",
    "        s = r\"${} \\times 10^{}$\".format(num, pwr)\n",
    "    else:\n",
    "        num = \"{\" + str(num)[2+n_decim] + \"}\"  # Truncate n_decim after the dot\n",
    "        s = r\"${} \\times 10^{}$\".format(num, pwr)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider doing separate panels for each antigen if the graphs become too overcrowded\n",
    "# Plot cytokine time series statistics\n",
    "def plot_cyto_timeseries_grid_stats(df_cy, grouplvl=\"TheoreticalAntigen\", pep_sel=None, palet=None, pep_ec50s=None, \n",
    "                                   feature=\"concentration\"):\n",
    "    nice_cytos_lbls = {\"IFNg\": r\"IFN-$\\gamma$\", \"TNFa\": \"TNF\"}  \n",
    "    # Get the number of antigen classes\n",
    "    if pep_sel is None:\n",
    "        pep_sel = df_cy.index.get_level_values(grouplvl).unique()\n",
    "        \n",
    "    # Compute the average and variance of each cytokine time series for each antigen class\n",
    "    df_means = df_cy.groupby([grouplvl, \"Time\"]).mean()\n",
    "    df_varis = df_cy.groupby([grouplvl, \"Time\"]).var()\n",
    "    \n",
    "    # Prepare plots\n",
    "    cytos = df_cy.columns.to_list()\n",
    "    ncols = len(pep_sel)\n",
    "    nrows = len(cytos)\n",
    "    fig, axes = plt.subplots(nrows, ncols, sharex=True, sharey=\"row\")\n",
    "    fig.set_size_inches(0.9*ncols, 0.9*nrows)\n",
    "\n",
    "    # Get the time points\n",
    "    times = df_cy.index.get_level_values(\"Time\").unique().to_list()\n",
    "    times.sort()\n",
    "    \n",
    "    # Prepare a color palette, if one was not already provided\n",
    "    if palet is None:\n",
    "        palet = sns.color_palette(n_colors=len(pep_sel))\n",
    "        palet = {k:palet[i] for i, k in enumerate(pep_sel)}\n",
    "    \n",
    "    # Plot each cytokine for each peptide\n",
    "    leghandles, leglabels = [], []\n",
    "    for j, ky in enumerate(pep_sel):\n",
    "        if pep_ec50s is not None:\n",
    "            ec50 = pep_ec50s.loc[ky, \"log10ec50\"]\n",
    "            label_ec50 = format_scinotation(ec50)\n",
    "        else:\n",
    "            label_ec50 = ky\n",
    "        for i in range(len(cytos)):\n",
    "            mn = df_means.loc[(ky, times), cytos[i]]\n",
    "            std = np.sqrt(df_varis.loc[(ky, times), cytos[i]])\n",
    "            li, = axes[i, j].plot(times, mn, label=label_ec50, color=palet[ky])\n",
    "            axes[i, j].plot(times, mn-std, lw=1., ls=\"--\", color=palet[ky], alpha=0.5)\n",
    "            axes[i, j].plot(times, mn+std, lw=1., ls=\"--\", color=palet[ky], alpha=0.5)\n",
    "            axes[i, j].fill_between(times, mn-std, mn+std,  color=palet[ky], alpha=0.2)\n",
    "            if i == 0:\n",
    "                leghandles.append(li)\n",
    "                leglabels.append(label_ec50)\n",
    "                axes[i, j].set_title(r\"EC$_{50}$: \" + label_ec50, fontsize=7)\n",
    "\n",
    "    # Some labeling\n",
    "    for i in range(len(cytos)):\n",
    "        ylbl = nice_cytos_lbls.get(cytos[i], cytos[i])\n",
    "        if feature == \"concentration\":\n",
    "            ylbl = ylbl + \" (pM)\"\n",
    "            ylblsize=None\n",
    "        elif feature == \"integral\":\n",
    "            ylbl = r\"$\\int_0^t \\mathrm{d}u \\, \\log_{10}[$\" + ylbl + r\"$]$\"\n",
    "            ylblsize = 6\n",
    "        axes[i, 0].set_ylabel(ylbl, size=ylblsize)\n",
    "    for j in range(ncols):\n",
    "        axes[-1, j].set_xlabel(\"Time (h)\")\n",
    "    \n",
    "    # Using logarithmic ticks\n",
    "    if feature == \"concentration\":\n",
    "        for i in range(len(cytos)):\n",
    "            ax = axes[i, 0]\n",
    "            cytlims = ax.get_ylim()\n",
    "            minorticks = compute_log_minor_ticks(cytlims, stp=1, base=10.0)\n",
    "            y_ticker = mpl.ticker.FuncFormatter(lambda x, pos:r\"$10^{}$\".format(\"{\"+str(int(x))+\"}\"))\n",
    "            ax.yaxis.set_major_formatter(y_ticker)\n",
    "            skp = (cytlims[1] - cytlims[0]) // 5 + 1\n",
    "            majoryticks = np.arange(np.ceil(cytlims[0]).astype(int), np.floor(cytlims[1]).astype(int)+1, skp)\n",
    "            if len(majoryticks) == 1:\n",
    "                majoryticks = np.arange(np.ceil(cytlims[0]).astype(int), np.floor(cytlims[1]).astype(int)+2, skp)\n",
    "            ax.set_yticks(majoryticks)\n",
    "            ax.set_yticks(minorticks, minor=True)\n",
    "            ax.tick_params(which=\"minor\", axis=\"both\", **props_minorticks)\n",
    "            ax.tick_params(which=\"major\", axis=\"both\", **props_majorticks)\n",
    "\n",
    "    # Decide where to place the legend\n",
    "    leg_esthetics = dict(frameon=False, title=r\"Antigen EC$_{50}$\", fontsize=7, borderpad=0.2, \n",
    "                        labelspacing=0.3, handletextpad=0.5, borderaxespad=0.3)\n",
    "    leg = fig.legend(leghandles, leglabels,\n",
    "                        bbox_to_anchor=(0.92, 0.5), loc=\"center left\", **leg_esthetics)\n",
    "    fig.tight_layout(w_pad=1.0)\n",
    "    fig.subplots_adjust(right=0.92)\n",
    "    return df_means, df_varis, fig, axes, leg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retn = plot_cyto_timeseries_grid_stats(df_recon_combined[\"concentration\"], \n",
    "                grouplvl=\"TheoreticalAntigen\", pep_sel=None, palet=colors_dict, \n",
    "                pep_ec50s=df_agclass_ec50s)\n",
    "_, _, fig, axes, leg = retn\n",
    "fig.savefig(os.path.join(main_dir_path, \"figures\", \"reconstruction\", \n",
    "            \"cytokine_reconstructions_theoretical_antigen_classes_multipanel.pdf\"), \n",
    "            transparent=True, bbox_inches=\"tight\", bbox_extra_artists=(leg,))\n",
    "plt.show()\n",
    "plt.close()\n",
    "del retn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrals too. Integrals obtained by time integration of the reconstructed cytokines. \n",
    "retn = plot_cyto_timeseries_grid_stats(df_recon_combined[\"integral\"], \n",
    "                grouplvl=\"TheoreticalAntigen\", pep_sel=None, palet=colors_dict, \n",
    "                pep_ec50s=df_agclass_ec50s, feature=\"integral\")\n",
    "df_recon_int_means, df_recon_int_varis, fig, axes, leg = retn\n",
    "plt.show()\n",
    "plt.close()\n",
    "del retn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
