{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training classifiers with IL-4 and IL-10 included\n",
    "We train on the time integrals of the logarithms of the normalized cytokine concentrations, as we do by default. \n",
    "We now include cytokines IL-4 and IL-10. The goal is to show that:\n",
    " 1. The structure of the latent space is essentially unchanged\n",
    " 2. The classifier's accuracy is not significantly higher even on training data\n",
    " 3. These cytokines are highly variable from experiment to experiment\n",
    "\n",
    "Either the variability makes the prediction very poor on some datasets, or the classifier learns to ignore them (by giving the low weights). In any case, there's a way to explain they are not important. \n",
    "\n",
    "It remains to be seen if those hypotheses are correct. The code below checks the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import psutil, pickle\n",
    "import itertools\n",
    "import os, sys\n",
    "main_dir_path = os.path.abspath('../')\n",
    "sys.path.insert(0, main_dir_path)\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "\n",
    "# Processing: min-max scaling of the data\n",
    "from sklearn.preprocessing import PowerTransformer, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Custom scripts\n",
    "import utils.custom_pandas as cpd\n",
    "from utils.mi_time_window import compute_mi_timecourse\n",
    "from utils.discrete_continuous_info import discrete_continuous_info_fast\n",
    "from utils.process_raw_data_choices import process_file_choices, select_naive_data\n",
    "from utils.multiprocess_training import slice_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU count for multiprocessing with the right number of jobs\n",
    "cpu_count = psutil.cpu_count(logical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Plot parameters for Science\n",
    "plt.rcParams[\"figure.figsize\"] = (2.5, 2.)\n",
    "plt.rcParams[\"axes.labelsize\"] = 8.\n",
    "plt.rcParams[\"legend.fontsize\"] = 8.\n",
    "plt.rcParams[\"axes.labelpad\"] = 0.5\n",
    "plt.rcParams[\"xtick.labelsize\"] = 7.\n",
    "plt.rcParams[\"ytick.labelsize\"] = 7.\n",
    "plt.rcParams[\"legend.title_fontsize\"] = 8.\n",
    "plt.rcParams[\"axes.titlesize\"] = 8.\n",
    "plt.rcParams[\"font.size\"] = 8.\n",
    "\n",
    "# For larger display of small graphs in the notebook\n",
    "plt.rcParams['figure.dpi'] = 120"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Available datasets\n",
    "'Activation_2', 'Activation_TCellNumber_1', 'Activation_Timeseries_1',\n",
    "       'CD25MutantTimeSeries_OT1_Timeseries_2', 'HighMI_1-1', 'HighMI_1-2',\n",
    "       'HighMI_1-3', 'HighMI_1-4',\n",
    "       'PeptideComparison_OT1_Timeseries_18',\n",
    "       'PeptideComparison_OT1_Timeseries_19',\n",
    "       'NewPeptideComparison_OT1_Timeseries_20',\n",
    "       'PeptideComparison_OT1_Timeseries_21',\n",
    "       'PeptideComparison_OT1_Timeseries_22',\n",
    "       'PeptideComparison_OT1_Timeseries_23',\n",
    "       'PeptideTumorComparison_OT1_Timeseries_1', 'TCellNumber_2',\n",
    "       'TCellNumber_OT1_Timeseries_7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and process cytokine data\n",
    "train_sets = [\n",
    "    'PeptideComparison_OT1_Timeseries_18',\n",
    "    'PeptideComparison_OT1_Timeseries_19',\n",
    "    'NewPeptideComparison_OT1_Timeseries_20',\n",
    "    'PeptideTumorComparison_OT1_Timeseries_1',\n",
    "    'TCellNumber_OT1_Timeseries_7', \n",
    "    'Activation_Timeseries_1'\n",
    "]\n",
    "df_train = {}\n",
    "for f in train_sets:\n",
    "    df_train[f] = select_naive_data(pd.read_hdf(os.path.join(main_dir_path, \"data\", \"processed\", f+\".hdf\")))\n",
    "df_train = pd.concat(df_train, names=[\"Data\"])\n",
    "\n",
    "# Select only integrals\n",
    "df_train = df_train.xs(\"integral\", level=\"Feature\", axis=1)\n",
    "# Select only the training peptides\n",
    "training_peps = [\"N4\", \"Q4\", \"T4\", \"V4\", \"G4\", \"E1\"]\n",
    "df_train = df_train.loc[df_train.index.isin(training_peps, level=\"Peptide\")]\n",
    "\n",
    "test_sets = [\n",
    "    #'Activation_2',  # Does not line up with usual data\n",
    "    'Activation_TCellNumber_1', \n",
    "    'CD25MutantTimeSeries_OT1_Timeseries_2',\n",
    "    #'PeptideComparison_OT1_Timeseries_21',  # Does not line up\n",
    "    'PeptideComparison_OT1_Timeseries_22',\n",
    "    'PeptideComparison_OT1_Timeseries_23',\n",
    "    'HighMI_1-1', 'HighMI_1-2',\n",
    "    'HighMI_1-3', 'HighMI_1-4'\n",
    "]\n",
    "df_test = {}\n",
    "for f in test_sets:\n",
    "    df_test[f] = select_naive_data(pd.read_hdf(os.path.join(main_dir_path, \"data\", \"processed\", f+\".hdf\")))\n",
    "df_test = pd.concat(df_test, names=[\"Data\"])\n",
    "\n",
    "df_test = df_test.xs(\"integral\", level=\"Feature\", axis=1)\n",
    "# Select only the training peptides\n",
    "df_test = df_test.loc[df_test.index.isin(training_peps, level=\"Peptide\")]\n",
    "\n",
    "allpeps_decreasing_qual = [\"N4\", \"A2\", \"Y3\", \"Q4\", \"T4\", \"Q7\", \"A8\", \"V4\", \"G4\", \"E1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables common to the two functions below\n",
    "# Put them in one initialization function called by each other function\n",
    "# to get a consistent set of peptides, concentrations and cytokines. \n",
    "def init_peps_cytos_concs():\n",
    "    # Keep the train_peptides order as in the original code. E1, weakest, is first.\n",
    "    train_peptides = [\"N4\", \"Q4\", \"T4\", \"V4\", \"G4\", \"E1\"][::-1]\n",
    "    keep_cytokines = [\"IFNg\", \"IL-17A\", \"IL-2\", \"IL-6\", \"TNFa\", \"IL-4\", \"IL-10\"]\n",
    "    keep_conc = [\"1uM\", \"100nM\", \"10nM\", \"1nM\"]\n",
    "    keep_cytokines.sort()\n",
    "    return train_peptides, keep_cytokines, keep_conc\n",
    "\n",
    "def train_classifier(data, hidden_sizes=(2,), seed=None, activ=\"tanh\"):\n",
    "    train_peptides, keep_cytokines, keep_conc = init_peps_cytos_concs()\n",
    "    peptide_dict = {k:v for v, k in enumerate(train_peptides)\n",
    "                         if k in data.index.get_level_values(\"Peptide\").unique()}\n",
    "\n",
    "    #Extract times and set classes\n",
    "    y = data.index.get_level_values(\"Peptide\").map(peptide_dict)\n",
    "\n",
    "    mlp = MLPClassifier(activation=activ, hidden_layer_sizes=hidden_sizes, max_iter=5000,\n",
    "            solver=\"adam\", random_state=seed, learning_rate=\"adaptive\", alpha=0.01).fit(data, y)\n",
    "\n",
    "    score = mlp.score(data, y)\n",
    "    return mlp, score\n",
    "\n",
    "\n",
    "def test_classifier(mlp, data):\n",
    "    train_peptides, keep_cytokines, keep_conc = init_peps_cytos_concs()\n",
    "    peptide_dict = {k:v for v, k in enumerate(train_peptides)\n",
    "                         if k in data.index.get_level_values(\"Peptide\").unique()}\n",
    "\n",
    "    #Extract times and set classes\n",
    "    y = data.index.get_level_values(\"Peptide\").map(peptide_dict)\n",
    "\n",
    "    score = mlp.score(data, y)\n",
    "    return score\n",
    "\n",
    "\n",
    "def test_classifier_bootstrap(mlp, data, n_rep=100):\n",
    "    train_peptides, keep_cytokines, keep_conc = init_peps_cytos_concs()\n",
    "    peptide_dict = {k:v for v, k in enumerate(train_peptides)\n",
    "                         if k in data.index.get_level_values(\"Peptide\").unique()}\n",
    "\n",
    "    # Make many bootstrap replicates by selecting 80 % of the test data\n",
    "    y = data.index.get_level_values(\"Peptide\").map(peptide_dict).to_series()\n",
    "    scores = []\n",
    "    for i in range(n_rep):\n",
    "        chosen = np.random.choice(np.arange(len(y)), replace=True, size=len(y))\n",
    "        x_boot = data.iloc[chosen]\n",
    "        y_boot = y.iloc[chosen]\n",
    "        scores.append(mlp.score(x_boot, y_boot))\n",
    "    return np.asarray(scores)\n",
    "\n",
    "def crossvalidate_classifier(data, hidden_sizes=(2,), seed=None, activ=\"tanh\"):\n",
    "    train_peptides, keep_cytokines, keep_conc = init_peps_cytos_concs()\n",
    "    peptide_dict = {k:v for v, k in enumerate(train_peptides)\n",
    "                         if k in data.index.get_level_values(\"Peptide\").unique()}\n",
    "\n",
    "    #Extract times and set classes\n",
    "    y = data.index.get_level_values(\"Peptide\").map(peptide_dict)\n",
    "\n",
    "    mlp = MLPClassifier(activation=activ, hidden_layer_sizes=hidden_sizes, max_iter=5000,\n",
    "            solver=\"adam\", random_state=seed, learning_rate=\"adaptive\", alpha=0.01)\n",
    "\n",
    "    scores = cross_validate(mlp, data.values, y, cv=5, return_train_score=True, return_estimator=False)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def process_train_dsets(df, tslice=slice(1, 71)):\n",
    "    train_peptides, keep_cytokines, keep_concs = init_peps_cytos_concs()\n",
    "    # Keep relevant cytokines only\n",
    "    df = df.loc[:, df.columns.isin(keep_cytokines, level=\"Cytokine\")]\n",
    "    # Keep training peptides only\n",
    "    df = df.loc[df.index.isin(train_peptides[::-1], level=\"Peptide\")]\n",
    "    # Keep 100k T cells only, which was done manually in the original network's training\n",
    "    df = df.xs(\"100k\", level=\"TCellNumber\", drop_level=False)\n",
    "    # Keep typical concentrations\n",
    "    df = df.loc[df.index.isin(keep_concs, level=\"Concentration\")]\n",
    "    df = slice_level(df, tslice, target_lvl=\"Time\")\n",
    "    # Normalize\n",
    "    dfmin, dfmax = df.min(axis=0), df.max(axis=0)\n",
    "    df = (df - dfmin) / (dfmax - dfmin)\n",
    "    dfminmax = pd.concat({\"min\": dfmin, \"max\":dfmax}, names=[\"Extremum\"], axis=1)\n",
    "    # Return the normalization factors too, to be able to reverse the scaling afterwards\n",
    "    return df, dfminmax\n",
    "\n",
    "def process_test_dsets(df, dfminmax, tslice=slice(1, 71)):\n",
    "    train_peptides, keep_cytokines, keep_concs = init_peps_cytos_concs()\n",
    "    # Keep relevant cytokines only\n",
    "    df = df.loc[:, df.columns.isin(keep_cytokines, level=\"Cytokine\")]\n",
    "    # Keep training peptides only\n",
    "    df = df.loc[df.index.isin(train_peptides[::-1], level=\"Peptide\")]\n",
    "    # Keep 100k T cells only, which was done manually in the original network's training\n",
    "    df = df.xs(\"100k\", level=\"TCellNumber\", drop_level=False)\n",
    "    # Keep typical concentrations\n",
    "    df = df.loc[df.index.isin(keep_concs, level=\"Concentration\")]\n",
    "    df = slice_level(df, tslice, target_lvl=\"Time\")\n",
    "    # Normalize with the training min and max\n",
    "    dfmin, dfmax = dfminmax[\"min\"], dfminmax[\"max\"]\n",
    "    df = (df - dfmin) / (dfmax - dfmin)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "train_time_slice = slice(1, 71)\n",
    "df_train_norm, df_minmax = process_train_dsets(df_train, tslice=train_time_slice)\n",
    "\n",
    "# Now train and cross-validate a classifier\n",
    "sd = 998243\n",
    "print(\"Starting training...\")\n",
    "classif, train_score = train_classifier(df_train_norm, hidden_sizes=(2,), seed=sd, activ=\"tanh\")\n",
    "print(\"Finished training once; training score: {}\".format(100*train_score))\n",
    "print(\"Starting cross-validation...\")\n",
    "cv_scores = crossvalidate_classifier(df_train_norm, hidden_sizes=(2,), seed=None, activ=\"tanh\")\n",
    "print(\"Cross-validation scores:\", cv_scores[\"test_score\"].mean(), \"pm\", cv_scores[\"test_score\"].std())\n",
    "print(\"Train scores during crossval:\", cv_scores[\"train_score\"].mean(), \"pm\", cv_scores[\"train_score\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "df_test_norm = process_test_dsets(df_test, df_minmax)\n",
    "test_scores = test_classifier_bootstrap(classif, df_test_norm)\n",
    "print(\"Test score:\", test_scores.mean(), \"pm\", test_scores.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the test score is equal to the usual test score when not including IL-4 and IL-10. \n",
    "Refer the reader to the heatmap cube showing  scores for log-smooth-integral-processed data and having $\\sim$65 % for the test score, although it was slightly lower for train ($\\sim$71 %). \n",
    "This clearly means the classifier overfits on IL-4/IL-10 noise! \n",
    "Even the cross-validation score above means there is overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare to not including IL-4 and IL-10\n",
    "Re-run this training here to make a plot summarizing the two types of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "train_time_slice = slice(1, 71)\n",
    "df_train_5 = df_train.loc[:, [\"IFNg\", \"IL-17A\", \"IL-2\", \"IL-6\", \"TNFa\"]]\n",
    "df_train_norm_5, df_minmax_5 = process_train_dsets(df_train_5, tslice=train_time_slice)\n",
    "\n",
    "# Now train and cross-validate a classifier\n",
    "sd = 90\n",
    "print(\"Starting training...\")\n",
    "classif_5, train_score_5 = train_classifier(df_train_norm_5, hidden_sizes=(2,), seed=sd, activ=\"tanh\")\n",
    "print(\"Finished training once; training score: {}\".format(100*train_score_5))\n",
    "print(\"Starting cross-validation...\")\n",
    "cv_scores_5 = crossvalidate_classifier(df_train_norm_5, hidden_sizes=(2,), seed=None, activ=\"tanh\")\n",
    "print(\"Cross-validation scores:\", cv_scores_5[\"test_score\"].mean(), \"pm\", cv_scores_5[\"test_score\"].std())\n",
    "print(\"Train scores during crossval:\", cv_scores_5[\"train_score\"].mean(), \"pm\", cv_scores_5[\"train_score\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "df_test_5 = df_test.loc[:, [\"IFNg\", \"IL-17A\", \"IL-2\", \"IL-6\", \"TNFa\"]]\n",
    "df_test_norm_5 = process_test_dsets(df_test_5, df_minmax_5)\n",
    "test_scores_5 = test_classifier_bootstrap(classif_5, df_test_norm_5)\n",
    "print(\"Test score:\", test_scores_5.mean(), \"pm\", test_scores_5.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot for scores comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nomograph with train, cross-validation, and test scores\n",
    "cv_scores[\"all_train_score\"] = np.concatenate([cv_scores[\"train_score\"], np.asarray([train_score])])\n",
    "all_scores_7 = np.asarray([cv_scores[\"all_train_score\"].mean(), cv_scores[\"test_score\"].mean(), test_scores.mean()])\n",
    "errors_scores_7 = np.asarray([cv_scores[\"all_train_score\"].std(), cv_scores[\"test_score\"].std(), test_scores.std()])\n",
    "\n",
    "cv_scores_5[\"all_train_score\"] = np.concatenate([cv_scores_5[\"train_score\"], np.asarray([train_score_5])])\n",
    "all_scores_5 = np.asarray([cv_scores_5[\"all_train_score\"].mean(), cv_scores_5[\"test_score\"].mean(), test_scores_5.mean()])\n",
    "errors_scores_5 = np.asarray([cv_scores_5[\"all_train_score\"].std(), cv_scores_5[\"test_score\"].std(), test_scores_5.std()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "color1, color2 = \"xkcd:coral\", \"xkcd:royal blue\"\n",
    "xticks = np.arange(len(all_scores_7))\n",
    "ax.errorbar(xticks, all_scores_7*100, yerr=errors_scores_7*100, marker=\"o\", ms=6, \n",
    "            color=color1, label=\"With IL-4 & IL-10\", mfc=color1, mec=color1)\n",
    "ax.errorbar([xticks[0], xticks[1]+0.02, xticks[2]+0.02], all_scores_5*100, yerr=errors_scores_5*100, marker=\"o\", ms=6,\n",
    "            color=color2, mfc=color2, mec=color2, label=\"No IL-4 & IL-10\")\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels([\"Train\", \"Cross-validation\", \"Test\"])\n",
    "ax.set_ylabel(\"Score (%)\")\n",
    "ax.legend(frameon=True, edgecolor=(1, 1, 1, 0), framealpha=0.9)\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.savefig(os.path.join(main_dir_path, \"figures\", \"latentspaces\", \n",
    "# \"l4_il10_train_crossval_test_scores.pdf\"), transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_to_latent(df_cy, classif, apply_offsets=False, apply_tanh=False):\n",
    "    projmat = classif.coefs_[0]  # 5x2 so dot on the right of data\n",
    "    # Apply tanh function and offsets, optionally (False by default)\n",
    "    offsets = classif.intercepts_[0]\n",
    "    \n",
    "    df_ls = df_cy.dot(projmat)\n",
    "    df_ls.columns = pd.Index([\"LS1\", \"LS2\"], name=\"Latent variable\")\n",
    "    if apply_offsets:\n",
    "        df_ls += offsets.reshape(1, -1)\n",
    "    if apply_tanh:\n",
    "        df_ls = np.tanh(df_ls)\n",
    "    return df_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With one flip and a rotation we can always get the desired orientation. \n",
    "# Flip if the initial angle for N4 is smaller (counterclockwise) than for other peptides\n",
    "# Then rotate by 0, 90, 180 or 270 until N4 trajectories have positive LS1 and LS2 initial slope. \n",
    "# Return the transformed LS and the transformations. \n",
    "def determine_flip_rotate_latentspace(df_ls):\n",
    "    average_lines_th = df_ls.groupby([\"Peptide\"]).mean()\n",
    "    angles = np.arctan2(average_lines_th[\"LS2\"], average_lines_th[\"LS1\"]) % (2*np.pi)\n",
    "    delta_angles = angles[\"N4\"] - angles[\"Q4\"]\n",
    "    if delta_angles < -np.pi:\n",
    "        delta_angles += 2*np.pi\n",
    "    if delta_angles > np.pi:\n",
    "        delta_angles -= 2*np.pi\n",
    "    flip1 = 1 if delta_angles > 0 else -1\n",
    "    # The flip will change angles; determine number of rotations\n",
    "    # needed AFTER flipping node 1. \n",
    "    angles2 = np.arctan2(average_lines_th[\"LS2\"], average_lines_th[\"LS1\"]*flip1) % (2*np.pi)\n",
    "    # Put the slope of N4 back in upper right quadrant (0-pi/2)\n",
    "    number_rots_minus90 = int(angles2[\"N4\"] // (np.pi/2))\n",
    "\n",
    "    return (flip1, number_rots_minus90)\n",
    "\n",
    "def apply_flip_rotate_latentspace(df_ls, flip1, n_rotations90):\n",
    "    rotmat = np.asarray([[1, 0], \n",
    "                         [0, 1]])\n",
    "    # Right-hand side dot product with following gives clockwise rotation by 90 degrees\n",
    "    rot90 = np.asarray([[0, -1], \n",
    "                        [1,  0]])\n",
    "    for n in range(n_rotations90):\n",
    "        rotmat = rotmat.dot(rot90)\n",
    "    # Apply transforms\n",
    "    df_ls2 = df_ls.copy()\n",
    "    df_ls2[\"LS1\"] *= flip1\n",
    "    df_ls2 = df_ls2.dot(rotmat)\n",
    "    df_ls2.columns = df_ls.columns\n",
    "    return df_ls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training and test latent spaces\n",
    "df_ls_train = project_to_latent(df_train_norm, classif, apply_offsets=False, apply_tanh=False)\n",
    "# Determine flips and apply them to training data\n",
    "ls_flips = determine_flip_rotate_latentspace(df_ls_train)\n",
    "df_ls_train = apply_flip_rotate_latentspace(df_ls_train, *ls_flips)\n",
    "\n",
    "\n",
    "# Compute the latent projection of test data as well\n",
    "df_ls_test = project_to_latent(df_test_norm, classif, apply_offsets=False, apply_tanh=False)\n",
    "# Apply the necessary flip and rotation\n",
    "df_ls_test = apply_flip_rotate_latentspace(df_ls_test, *ls_flips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for the classifier using only 5 cytokines\n",
    "# Compute training and test latent spaces\n",
    "df_ls_train_5 = project_to_latent(df_train_norm_5, classif_5, apply_offsets=False, apply_tanh=False)\n",
    "# Determine flips and apply them to training data\n",
    "ls_flips_5 = determine_flip_rotate_latentspace(df_ls_train_5)\n",
    "df_ls_train_5 = apply_flip_rotate_latentspace(df_ls_train_5, *ls_flips_5)\n",
    "\n",
    "\n",
    "# Compute the latent projection of test data as well\n",
    "df_ls_test_5 = project_to_latent(df_test_norm_5, classif_5, apply_offsets=False, apply_tanh=False)\n",
    "# Apply the necessary flip and rotation\n",
    "df_ls_test_5 = apply_flip_rotate_latentspace(df_ls_test_5, *ls_flips_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# First check latent spaces without any flipping, offsets or tanh\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "axes = axes.flatten()\n",
    "fig.set_size_inches(2.25*2, 2.25)\n",
    "\n",
    "peps_in_df = [p for p in allpeps_decreasing_qual \n",
    "              if p in df_ls_train.index.get_level_values(\"Peptide\").unique()]\n",
    "g = sns.lineplot(data=df_ls_train.reset_index(), x=\"LS1\", y=\"LS2\", \n",
    "           hue=\"Peptide\", hue_order=peps_in_df, ax=axes[0], sort=False, sizes=[1.5, 1., 0.75, 0.5],\n",
    "           size=\"Concentration\", size_order=[\"1uM\", \"100nM\", \"10nM\", \"1nM\"],\n",
    "           style=\"Data\", legend=False)\n",
    "g = sns.lineplot(data=df_ls_train_5.reset_index(), x=\"LS1\", y=\"LS2\", \n",
    "           hue=\"Peptide\", hue_order=peps_in_df, ax=axes[1], sort=False, sizes=[1.5, 1., 0.75, 0.5],\n",
    "           size=\"Concentration\", size_order=[\"1uM\", \"100nM\", \"10nM\", \"1nM\"],\n",
    "           style=\"Data\", legend=False)\n",
    "for i in range(2):\n",
    "    axes[i].set(xlabel=r\"LS$_1$ (a.u.)\", ylabel=r\"LS$_2$ (a.u.)\", \n",
    "           xticks=[], xticklabels=[], yticks=[], yticklabels=[])\n",
    "    axes[i].spines[\"top\"].set_visible(False)\n",
    "    axes[i].spines[\"right\"].set_visible(False)\n",
    "axes[0].set_title(\"With IL-4 & IL-10\")\n",
    "axes[1].set_title(\"Without IL-4 & IL-10\")\n",
    "\n",
    "fig.tight_layout()\n",
    "# fig.savefig(os.path.join(main_dir_path, \"figures\", \"latentspaces\", \n",
    "#    \"il4_il10_latent_spaces_trained.pdf\"), transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal-to-noise ratio\n",
    "We compute the signal-to-noise ratio of each cytokine in another Jupyter notebook, ``cytokine_noise_distribution.ipynb``, where we also study the distribution of cytokines themselves and of the noise (variability around fitted splines) in linear and log scale. From those distributions, it is very easy to compute the SNR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
