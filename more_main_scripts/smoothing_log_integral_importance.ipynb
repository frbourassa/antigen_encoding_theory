{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training classifiers with different input processings\n",
    "The goal is to check whether log transform, smoothing, and time integration are important for more accurate predictions. So here we systematically check the $2^3 = 8$ possible combinations of log, smoothing (and interpolation), and integration in processing cytokine data before classification. \n",
    "\n",
    "We stick to the same training data as with the original latent space, for better comparison. We even use the same seed in the case where all processings are on, to recover the same neural network. \n",
    "\n",
    "We always normalize the data with the min and max of the training set (min-max scaling). The min and max of course change depending on whether we take the log or not, and whether we time-integrate or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import scipy.stats\n",
    "import psutil, pickle, json\n",
    "import itertools\n",
    "import multiprocessing\n",
    "import os, sys\n",
    "main_dir_path = os.path.abspath('../')\n",
    "if main_dir_path not in sys.path:\n",
    "    sys.path.insert(0, main_dir_path)\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Custom scripts\n",
    "import utils.custom_pandas as cpd\n",
    "from metrics.mi_time_window import compute_mi_timecourse\n",
    "from metrics.discrete_continuous_info import discrete_continuous_info_fast\n",
    "from utils.process_raw_data_choices import process_file_choices, select_naive_data\n",
    "from utils.multiprocess_training import (slice_level, process_train_dsets, train_classifier,  process_test_dsets, \n",
    "                                crossvalidate_classifier, test_classifier, init_peps_cytos_concs, process_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU count for multiprocessing with the right number of jobs\n",
    "cpu_count = psutil.cpu_count(logical=False)\n",
    "multiprocessing.set_start_method(\"spawn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label sizes for Science format (figure width 2.25 inches or 4.75 inches)\n",
    "# Squeezing three subplots in a row: 4.75/3 = 1.583333\n",
    "sns.reset_orig()\n",
    "plt.rcParams[\"figure.figsize\"] = (1.55, 1.65)\n",
    "plt.rcParams[\"font.size\"] = 8\n",
    "plt.rcParams[\"axes.labelsize\"] = 7\n",
    "plt.rcParams[\"legend.fontsize\"] = 7\n",
    "plt.rcParams[\"xtick.labelsize\"] = 6\n",
    "plt.rcParams[\"ytick.labelsize\"] = 6\n",
    "plt.rcParams[\"xtick.major.pad\"] = 2.  # distance to major tick label in points\n",
    "plt.rcParams[\"xtick.minor.pad\"] = 2.\n",
    "plt.rcParams[\"axes.labelpad\"] = 1.\n",
    "plt.rcParams[\"axes.linewidth\"] = 0.8\n",
    "#plt.rcParams[\"axes.spines.top\"] = False\n",
    "#plt.rcParams[\"axes.spines.right\"] = False\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 150 # default for me was 75\n",
    "fs = 10\n",
    "\n",
    "with open(os.path.join(main_dir_path, \"data\", \"misc\", \"major_ticks_props.json\"), \"r\") as hd:\n",
    "    props_majorticks = json.load(hd)\n",
    "with open(os.path.join(main_dir_path, \"data\", \"misc\", \"minor_ticks_props.json\"), \"r\") as hd:\n",
    "    props_minorticks = json.load(hd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions for the different processing combinations\n",
    "The basic code in those functions copied from our processing pipeline, with options added here for cases where one or many processing steps are turned off. \n",
    "\n",
    "The code customized for this application is in utils/process_raw_data.py. The main function to process a dataframe is ``process_file_choices``. \n",
    "\n",
    "Some processing combinations are tricky:\n",
    "- Time integral without smoothing\n",
    "- Smoothing without log transform (because trajectories may look funny and be hard to de-noise)\n",
    "\n",
    "We pay special attention to them in ``process_file_choices``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpeps_decreasing_qual = [\"N4\", \"A2\", \"Y3\", \"Q4\", \"T4\", \"Q7\", \"A8\", \"V4\", \"G4\", \"E1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try processing for integrals without smoothing (most complicated case)\n",
    "process_kwargs = {\"take_log\": True, \"rescale_max\": False, \"max_time\": 96, \n",
    "                  \"do_integrate\": True, \"do_smooth\": False}\n",
    "dset_to_test = 'cytokineConcentrationPickleFile-20190718-PeptideComparison_4-final.hdf'\n",
    "\n",
    "df_processed = process_file_choices(os.path.join(main_dir_path, \"data\", \"final\"), dset_to_test, **process_kwargs)\n",
    "\n",
    "sns.relplot(data=df_processed[\"integral\"].reset_index(), x=\"Time\", y=\"IFNg\", \n",
    "            hue=\"Peptide\", style=\"TCellNumber\", size=\"Concentration\", kind=\"line\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the smoothing+splines when in linear scale. Try integrals and concentrations manually here. \n",
    "process_kwargs = {\"take_log\": False, \"rescale_max\": False, \"max_time\": 96, \n",
    "                  \"do_integrate\": False, \"do_smooth\": False}\n",
    "df_linear_conc = process_file_choices(os.path.join(main_dir_path, \"data\", \"final\"), dset_to_test, **process_kwargs)\n",
    "\n",
    "process_kwargs[\"do_smooth\"] = True\n",
    "df_linear_conc_smooth = process_file_choices(os.path.join(main_dir_path, \"data\", \"final\"), dset_to_test, **process_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train classifiers on each of the eight possible processing combinations\n",
    "We import the raw training datasets and clear any data that isn't naive OT-1 CD8+ T cells, for each of the eight processing combinations. Then, we train a MLP classifier with one hidden layer on each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_files = [\n",
    "    'cytokineConcentrationPickleFile-20190412-PeptideComparison_2-final.hdf',\n",
    "    'cytokineConcentrationPickleFile-20190608-PeptideComparison_3-final.hdf',\n",
    "    'cytokineConcentrationPickleFile-20190718-PeptideComparison_4-final.hdf',\n",
    "    'cytokineConcentrationPickleFile-20190725-PeptideComparison_5-final.hdf',\n",
    "    'cytokineConcentrationPickleFile-20190802-TCellNumber_1-final.hdf', \n",
    "    'cytokineConcentrationPickleFile-20190812-Activation_1-final.hdf'\n",
    "]\n",
    "train_data_names = [a[41:-10] for a in train_data_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Cover all combination of log-no log, smooth-no smooth, integral-no integral\n",
    "all_classifiers = {}\n",
    "all_train_dfs = {}\n",
    "all_minmax_dfs = {}\n",
    "all_train_scores_long = {}\n",
    "keys_lis = list(itertools.product([False, True], repeat=3))\n",
    "\n",
    "# Multiprocessing\n",
    "pool = multiprocessing.pool.Pool(processes=cpu_count)\n",
    "all_return_objs = []\n",
    "for lis in keys_lis:\n",
    "    re = pool.apply_async(process_train, \n",
    "            args=(lis, train_data_files, os.path.join(main_dir_path, \"data\", \"final\")))\n",
    "    all_return_objs.append(re)\n",
    "\n",
    "all_returns = [p.get() for p in all_return_objs]\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "for i in range(len(tuple(keys_lis))):\n",
    "    all_classifiers[tuple(keys_lis[i])] = all_returns[i][0]\n",
    "    all_train_scores_long[tuple(keys_lis[i])] = all_returns[i][1]\n",
    "    all_train_dfs[tuple(keys_lis[i])] = all_returns[i][2]\n",
    "    all_minmax_dfs[tuple(keys_lis[i])] = all_returns[i][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, tscore in all_train_scores_long.items():\n",
    "    print(k)\n",
    "    print(tscore[\"train_score\"].mean(), \"pm\", tscore[\"train_score\"].std())\n",
    "    print(tscore[\"test_score\"].mean(), \"pm\", tscore[\"test_score\"].std())\n",
    "    print(tscore[\"whole_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_scores = {k:all_train_scores_long[k][\"whole_score\"] for k in all_train_scores_long.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Validation scores: process training data and validate\n",
    "train_time_slice = slice(1, 71)\n",
    "test_data_files = [\n",
    "    'cytokineConcentrationPickleFile-20200220-TCellNumber_3-final.hdf', \n",
    "    'cytokineConcentrationPickleFile-20190404-PeptideComparison_1-final.hdf',\n",
    "    'cytokineConcentrationPickleFile-20191029-PeptideComparison_8-final.hdf',\n",
    "    'cytokineConcentrationPickleFile-20191106-PeptideComparison_9-final.hdf', \n",
    "    'cytokineConcentrationPickleFile-20200624-HighMI_1-1-final.hdf',\n",
    "    'cytokineConcentrationPickleFile-20200624-HighMI_1-2-final.hdf',\n",
    "    'cytokineConcentrationPickleFile-20200624-HighMI_1-3-final.hdf',\n",
    "    'cytokineConcentrationPickleFile-20200624-HighMI_1-4-final.hdf'\n",
    "]\n",
    "test_data_names = [a[41:-10] for a in test_data_files]\n",
    "\n",
    "# Use the training normalization coefficients!\n",
    "all_test_dfs = {}\n",
    "all_test_scores = {}\n",
    "for lis in itertools.product([False, True], repeat=3):  # l: log, s: smooth, i: integral\n",
    "    # Process test data according to lis\n",
    "    process_kwargs[\"take_log\"] = lis[0]\n",
    "    process_kwargs[\"do_smooth\"] = lis[2]\n",
    "    process_kwargs[\"do_integrate\"] = lis[1]\n",
    "    # Get the min-max scaling\n",
    "    df_minmax = all_minmax_dfs[tuple(lis)]\n",
    "    df_test = process_test_dsets(test_data_files, process_kwargs, df_minmax, \n",
    "                folder=os.path.join(main_dir_path, \"data\", \"final\"), tslice=train_time_slice)\n",
    "    print(lis)\n",
    "    print(\"Number of points:\", df_test.shape)  # For interpolated data, should be (5680, 5)\n",
    "    \n",
    "    # Use only time points > 12 hours\n",
    "    # df_test = df_test.loc[df_test.index.get_level_values(\"Time\") >= 12]\n",
    "    \n",
    "    # Get the appropriate MLP and test it\n",
    "    mlp = all_classifiers[tuple(lis)]\n",
    "    score = test_classifier(mlp, df_test)\n",
    "\n",
    "    print(\"Test score: {}\".format(100*score))\n",
    "    print()\n",
    "    \n",
    "    # Store the classifier, data, and normalization factors\n",
    "    all_test_scores[tuple(lis)] = score\n",
    "    all_test_dfs[tuple(lis)] = df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scores with the eight combinations\n",
    "# Stack of two heatmaps, based on this solution:\n",
    "# https://stackoverflow.com/questions/57502763/how-to-plot-horizontal-stack-of-heatmaps-or-a-stack-of-grid\n",
    "def heatmap_stack(scores_dict, cbar_label=\"Training score\", cmap_choice=\"plasma\", score_range=None, \n",
    "                 figax=None, add_cbar=True, lblsize=7):\n",
    "    # Create a cube array of scores\n",
    "    n = int(round(len(scores_dict.keys())**(1/3), 0))\n",
    "    cube_scores = np.zeros([n, n, n])\n",
    "    for lis in scores_dict.keys():\n",
    "        l, i, s = map(int, lis)\n",
    "        cube_scores[l, i, s] = scores_dict[lis]\n",
    "    print(cube_scores)\n",
    "    \n",
    "    if figax is None:\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(projection=\"3d\")\n",
    "    else:\n",
    "        fig, ax = figax\n",
    "    x_smooth, z_int = np.meshgrid([0, 1], [0, 1])\n",
    "    x_edges, z_edges = np.meshgrid(np.linspace(-0.5, 1.5, 3), np.linspace(-0.5, 1.5, 3))\n",
    "    y_log = np.asarray([0, 1])  # Index for stacks: log (first in lis)\n",
    "\n",
    "    cmap = mpl.cm.get_cmap(cmap_choice)\n",
    "    if score_range is None:\n",
    "        score_range = (np.amin(cube_scores), np.amax(cube_scores))\n",
    "    norm = mpl.colors.Normalize(vmin=score_range[0]*100, vmax=score_range[1]*100)\n",
    "\n",
    "    ## TODO: make custom cmap with 8 values for the eight scores? Easier to read? \n",
    "    for y in y_log:\n",
    "        colors = cmap(norm(cube_scores[y]*100))  # Transpose to go from ij to xy convention\n",
    "        ax.plot_surface(x_edges, np.zeros(shape=x_edges.shape)+y, z_edges, facecolors=colors, linewidth=0)\n",
    "\n",
    "    # Annotate the actual scores in each square\n",
    "    for lis in scores_dict.keys():\n",
    "        score = scores_dict[lis]\n",
    "        xyz = (int(lis[2]), int(lis[0]), int(lis[1]))\n",
    "        ax.text(*xyz, \"{:.1f} %\".format(score*100), ha=\"center\", va=\"center\", color=\"w\", zorder=100, fontsize=lblsize)\n",
    "\n",
    "    # Connect each square on one heatmap to the analog square on the other?\n",
    "    # Give a zorder in between the two surfaces. Doesn't work now, so just ignore. \n",
    "    #for si in itertools.product([0, 1], repeat=2):\n",
    "    #    ax.plot(xs=[si[0]]*2, ys=[0, 1], zs=[si[1]]*2, ls=\"-\", color=(0., 0., 0., 0.7), lw=3, zorder=100)\n",
    "\n",
    "\n",
    "    # Colorbar\n",
    "    # fig.subplots_adjust(right=0.6)\n",
    "    if add_cbar:\n",
    "        fig.colorbar(mpl.cm.ScalarMappable(norm, cmap=cmap_choice), ax=[ax], location='left', \n",
    "                anchor=(1.0, 0.25), shrink=0.7, aspect=10, label=cbar_label + \" (%)\")\n",
    "\n",
    "    # Labeling axes and ticks\n",
    "    lblnoyes = [\"No\",\" Yes\"]\n",
    "    ax.set_xlabel(\"Smoothing\", labelpad=-10)\n",
    "    ax.set_xticks(x_smooth[0])\n",
    "    ax.set_xticklabels(lblnoyes)\n",
    "\n",
    "    ax.set_ylabel(\"Log transform\", labelpad=-10)\n",
    "    ax.set_yticks(y_log)\n",
    "    ax.set_yticklabels(lblnoyes)\n",
    "\n",
    "    ax.set_zlabel(\"Integral\", labelpad=-10)\n",
    "    ax.set_zticks(z_int[:, 0])\n",
    "    ax.set_zticklabels(lblnoyes)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", pad=-4)\n",
    "    \n",
    "    ax.view_init(azim=130, elev=30)\n",
    "    return fig, ax, cube_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function for colorbars\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "def add_colorbar(mappable, ax, **kwargs):\n",
    "    \"\"\" Taken from https://joseph-long.com/writing/colorbars/ \"\"\"\n",
    "    fig = ax.figure\n",
    "    divider = make_axes_locatable(ax)\n",
    "    cax = divider.append_axes(\"left\", size=\"3%\", pad=0.05)\n",
    "    return fig.colorbar(mappable, cax=cax, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combined plot\n",
    "score_max = max(*all_train_scores.values(), *all_test_scores.values())\n",
    "score_min = min(*all_train_scores.values(), *all_test_scores.values())\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(1.5*4, 2.75)\n",
    "gs = fig.add_gridspec(1, 9)  # Give first ninth of the plot to color bar\n",
    "ax_cbar = fig.add_subplot(gs[0:1])\n",
    "ax_cbar.set_axis_off()\n",
    "ax_train = fig.add_subplot(gs[1:5], projection=\"3d\")\n",
    "ax_test = fig.add_subplot(gs[5:9], projection=\"3d\")\n",
    "fig.subplots_adjust(wspace=2.5)\n",
    "\n",
    "ax_train.set_title(\"Training\", fontsize=8)\n",
    "ax_test.set_title(\"Testing\", fontsize=8)\n",
    "\n",
    "cmap_chosen = \"plasma\"\n",
    "fig, ax_train, cube_train_scores = heatmap_stack(all_train_scores, cbar_label=\"Score\", cmap_choice=cmap_chosen,\n",
    "                        score_range=(score_min, score_max), figax=[fig, ax_train], add_cbar=False, lblsize=7)\n",
    "fig, ax_test, cube_test_scores = heatmap_stack(all_test_scores, cbar_label=\"Score\", cmap_choice=cmap_chosen,\n",
    "                        score_range=(score_min, score_max), figax=[fig, ax_test], add_cbar=False, lblsize=7)\n",
    "\n",
    "# Add colorbar at the end, in a separate axes. Otherwise one plot is smaller\n",
    "norm = mpl.colors.Normalize(vmin=score_min*100, vmax=score_max*100)\n",
    "cbar = fig.colorbar(mpl.cm.ScalarMappable(norm, cmap=cmap_chosen), ax=ax_cbar, fraction=1, aspect=10, \n",
    "                   label=\"Score (%)\", shrink=0.6)\n",
    "cbar.set_label=\"Score (%)\"\n",
    "\n",
    "#fig.tight_layout()\n",
    "\n",
    "#fig.savefig(os.path.join(main_dir_path, \"figures\", \"latentspaces\", \n",
    "#   \"train_test_heatcube_log-integral-smooth.pdf\"), transparent=True)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent spaces depending on processing\n",
    "Look at the latent spaces reached with each of the processing combinations. \n",
    "Organize plots on four rows of two plots. \n",
    "Make one column for logs, one for no-logs. \n",
    "\n",
    "Make sure to flip the latent spaces for each space until LS1 is increasing and LS2 is highest for N4. \n",
    "\n",
    "Define a couple of functions to order the eight combinations of processing in a logical way on a 2D grid. I use degree order with log being the most important, then integral, then smoothing (L-I-S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import cmp_to_key  # Convert a comparison function to a key class \n",
    "# (the key is applied to each element before sorting to redefine how they are compared)\n",
    "\n",
    "# Define a degree order relation between processing choices. \n",
    "# Cover one plane of constant sum at a time, then within that plane, \n",
    "# use the ordering log > smoothing > integral\n",
    "# Basically: swipe over triangles in the (+, +, +) octant\n",
    "# When the sum is equal, the first element is checked, then then second, etc. \n",
    "# by calling comparison recursively on sub-tuples\n",
    "# So we are assuming that the tuple is ordered in decreasing order of importance\n",
    "def degree_order(x, y):\n",
    "    # If the sum is different, the largest one wins\n",
    "    sx = sum(x)\n",
    "    sy = sum(y)\n",
    "    if sx != sy:\n",
    "        return sx - sy\n",
    "    elif x[0] != y[0]:\n",
    "        return x[0] - y[0]\n",
    "    # Recursion on remaining elements\n",
    "    elif len(x) > 1:\n",
    "        return degree_order(x[1:], y[1:])\n",
    "    elif len(x) == 1:  # They are equal because the first elements were not different\n",
    "        return 0\n",
    "    else:  # The code should never reach this point\n",
    "        raise ValueError(\"Recursive comparison failed\")\n",
    "\n",
    "key_degree_order = cmp_to_key(degree_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test this on our eight keys\n",
    "sorted(list(all_test_scores.keys()), key=key_degree_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_to_latent(df_cy, classif, apply_offsets=False, apply_tanh=False):\n",
    "    projmat = classif.coefs_[0]  # 5x2 so dot on the right of data\n",
    "    # Apply tanh function and offsets, optionally (False by default)\n",
    "    offsets = classif.intercepts_[0]\n",
    "    \n",
    "    df_ls = df_cy.dot(projmat)\n",
    "    df_ls.columns = pd.Index([\"LS1\", \"LS2\"], name=\"Latent variable\")\n",
    "    if apply_offsets:\n",
    "        df_ls += offsets.reshape(1, -1)\n",
    "    if apply_tanh:\n",
    "        df_ls = np.tanh(df_ls)\n",
    "    return df_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine whether the latent space of a classifier should be flipped\n",
    "# It doesn't perfectly work; see better version below. \n",
    "def find_flip_latentspace(df_ls):\n",
    "    dict_qualities = {allpeps_decreasing_qual[i]:i for i in range(len(allpeps_decreasing_qual))}\n",
    "    # Make sure N1 is increasing overall, with a linear regression over averages?\n",
    "    avg_ls1_over_time = df_ls.xs(\"N4\", level=\"Peptide\").loc[:, \"LS1\"].groupby(\"Time\").mean().sort_index()\n",
    "    res = sp.stats.linregress(avg_ls1_over_time.index.get_level_values(\"Time\").astype(float), \n",
    "                              avg_ls1_over_time.values)\n",
    "    flip_ls1 = 1 if res.slope >= 0 else -1\n",
    "    \n",
    "    # Make sure max LS2 for strongest peptide is higher than max LS2 for lowest\n",
    "    max_ls2 = df_ls.loc[:, \"LS2\"].groupby(\"Peptide\").max()\n",
    "    peps_sorted = sorted(max_ls2.index.get_level_values(\"Peptide\"), \n",
    "                         key=lambda x: dict_qualities.get(x, len(dict_qualities)+1))\n",
    "    flip_ls2 = 1 if max_ls2[peps_sorted[0]] >= max_ls2[peps_sorted[1]] else -1\n",
    "    return np.asarray([[flip_ls1, flip_ls2]])  # Will broadcast to cover every row\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With one flip and a rotation we can always get the desired orientation. \n",
    "# Flip if the initial angle for N4 is smaller (counterclockwise) than for other peptides\n",
    "# Then rotate by 0, 90, 180 or 270 until N4 trajectories have positive LS1 and LS2 initial slope. \n",
    "# Return the transformed LS and the transformations. \n",
    "def determine_flip_rotate_latentspace(df_ls):\n",
    "    average_lines_th = df_ls.groupby([\"Peptide\"]).mean()\n",
    "    angles = np.arctan2(average_lines_th[\"LS2\"], average_lines_th[\"LS1\"]) % (2*np.pi)\n",
    "    delta_angles = angles[\"N4\"] - angles[\"Q4\"]\n",
    "    if delta_angles < -np.pi:\n",
    "        delta_angles += 2*np.pi\n",
    "    if delta_angles > np.pi:\n",
    "        delta_angles -= 2*np.pi\n",
    "    flip1 = 1 if delta_angles > 0 else -1\n",
    "    # The flip will change angles; determine number of rotations\n",
    "    # needed AFTER flipping node 1. \n",
    "    angles2 = np.arctan2(average_lines_th[\"LS2\"], average_lines_th[\"LS1\"]*flip1) % (2*np.pi)\n",
    "    # Put the slope of N4 back in upper right quadrant (0-pi/2)\n",
    "    number_rots_minus90 = int(angles2[\"N4\"] // (np.pi/2))\n",
    "\n",
    "    return (flip1, number_rots_minus90)\n",
    "\n",
    "def apply_flip_rotate_latentspace(df_ls, flip1, n_rotations90):\n",
    "    rotmat = np.asarray([[1, 0], \n",
    "                         [0, 1]])\n",
    "    # Right-hand side dot product with following gives clockwise rotation by 90 degress\n",
    "    rot90 = np.asarray([[0, -1], \n",
    "                        [1,  0]])\n",
    "    for n in range(n_rotations90):\n",
    "        rotmat = rotmat.dot(rot90)\n",
    "    # Apply transforms\n",
    "    df_ls2 = df_ls.copy()\n",
    "    df_ls2[\"LS1\"] *= flip1\n",
    "    df_ls2 = df_ls2.dot(rotmat)\n",
    "    df_ls2.columns = df_ls.columns\n",
    "    return df_ls2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute training and test latent spaces\n",
    "all_classif_flips = {}\n",
    "all_train_latentspaces = {}\n",
    "all_test_latentspaces = {}\n",
    "for k in all_classifiers.keys():\n",
    "    # Compute original latent space\n",
    "    clf = all_classifiers[k]\n",
    "    df_ls_train = project_to_latent(all_train_dfs[k], clf, apply_offsets=False, apply_tanh=False)\n",
    "    # Determine if flips are necessary -- Old version\n",
    "    #all_classif_flips[k] = find_flip_latentspace(df_ls_train)\n",
    "    # Apply the flips\n",
    "    #df_ls_train *= all_classif_flips[k]\n",
    "    \n",
    "    # Determine flips and apply them to training data\n",
    "    all_classif_flips[k] = determine_flip_rotate_latentspace(df_ls_train)\n",
    "    df_ls_train = apply_flip_rotate_latentspace(df_ls_train, *all_classif_flips[k])\n",
    "    all_train_latentspaces[k] = df_ls_train\n",
    "    \n",
    "    # Compute the latent projection of test data as well\n",
    "    df_ls_test = project_to_latent(all_test_dfs[k], clf, apply_offsets=False, apply_tanh=False)\n",
    "    #df_ls_test *= all_classif_flips[k]\n",
    "    # Apply the necessary flip and rotation\n",
    "    df_ls_test = apply_flip_rotate_latentspace(df_ls_test, *all_classif_flips[k])\n",
    "    all_test_latentspaces[k] = df_ls_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First check latent spaces without any flipping, offsets or tanh\n",
    "fig, axes = plt.subplots(2, 4)\n",
    "fig.set_size_inches(1.5*4, 1.8*2)\n",
    "i0, i1 = 0, 0  # Where we are at in each column, depending on the sort order we choose\n",
    "for k in sorted(list(all_test_scores.keys()), key=key_degree_order):\n",
    "    j = 1 if k[0] else 0  # First column for no log\n",
    "    i = i1 if k[0] else i0  # Next row to fill in the correct column\n",
    "    ax = axes[j, i]\n",
    "    peps_in_df = [p for p in allpeps_decreasing_qual \n",
    "                  if p in all_train_latentspaces[k].index.get_level_values(\"Peptide\").unique()]\n",
    "    g = sns.lineplot(data=all_train_latentspaces[k].reset_index(), x=\"LS1\", y=\"LS2\", \n",
    "               hue=\"Peptide\", hue_order=peps_in_df, ax=ax, sort=False, sizes=[1.5, 1., 0.75, 0.5],\n",
    "               size=\"Concentration\", size_order=[\"1uM\", \"100nM\", \"10nM\", \"1nM\"],\n",
    "               style=\"Data\", legend=False)\n",
    "    ax.set(xlabel=r\"LS$_1$ (a.u.)\", ylabel=r\"LS$_2$ (a.u.)\", \n",
    "           xticks=[], xticklabels=[], yticks=[], yticklabels=[])\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    plot_title = \"Log\\n\" if k[0] else \"Linear\\n\" \n",
    "    plot_title += \"Integral\\n\" if k[1] else \"No integral\\n\"\n",
    "    plot_title += \"Smooth\" if k[2] else \"No smooth\"\n",
    "    fontweight = \"bold\" if k[1] and k[2] and k[0] else None\n",
    "    ax.set_title(plot_title, y=0.9, fontweight=fontweight, fontsize=8)\n",
    "    # Update i0 or i1\n",
    "    i0 += (1-j)  # 1 if j=0, else 0\n",
    "    i1 += j\n",
    "fig.tight_layout()\n",
    "#fig.savefig(os.path.join(main_dir_path, \"figures\", \"latentspaces\", \n",
    "#   \"latent_spaces_trained_log-integral-smooth.pdf\"), transparent=True, dpi=300)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credits\n",
    "Script and imported custom functions written by frbourassa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
