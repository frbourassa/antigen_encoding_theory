{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating cytokine trajectories from ballistic parameters\n",
    "TO run this notebook, you need:\n",
    "- Pre-processed cytokine time series in the `data/processed/` folder\n",
    "- the input weights of a neural network and the min and max cytokine concentrations used to scale the data, in `data/trained-networks`. \n",
    "Those files are available in the data repository hosted online, or you can generate them yourself from raw cytokine data using [`cytokine-pipeline`](https://github.com/tjrademaker/cytokine-pipeline). \n",
    "\n",
    "\n",
    "## Main steps to follow\n",
    "The goal is to have a model for cytokine dynamics. We want to show that we can create realistic time courses just by picking a few parameter values, corresponding to different ligand quality, ligand quantities, and T cell numbers. \n",
    "\n",
    "In particular, here, we fit kernel density estimates (KDEs) in model parameter space, sample from them, and compute the resulting latent and full-space trajectories. We then give those trajectories to Thomas for re-classification. We also compare them to trajectories from the data sets used to build the KDEs. \n",
    "\n",
    "1. Import selected WT data\n",
    "    1. Select HighMI dataset to train the reconstruction algorithm\n",
    "    2. Select datasets to fit parameter value KDEs\n",
    "2. Train the reconstruction algorithm\n",
    "3. Fit the sigmoid-ballistic model to all datasets in latent space\n",
    "4. Fit KDEs on the parameter space\n",
    "5. Sample from the KDEs to generate model (sigmoid-ballistic) latent space trajectories \n",
    "5. Project back those curves to cytokine concentration space, as well as those used to train the reconstruction\n",
    "7. Try also reconstructing all data sets used for parameter space, as a proof that we can uniformize them? \n",
    "    7.1 And look again at their re-projection in latent space after uniformization, as a proof they look alike more? \n",
    "8. Prepare a nice dataframe of synthetic, processed cytokine time courses for classification by Thomas' neural network. \n",
    "\n",
    "## Code structure\n",
    "\n",
    "The following useful functions are in separate Python scripts, for clarity of the notebook. \n",
    "\n",
    "- Scripts to import and process data: ltspcyt.scripts.neural_network\n",
    "- Scripts to train a reconstruction model: ltspcyt.scripts.reconstruction\n",
    "- Scripts to fit the sigmoid-ballistic model: ltspcyt.scripts.sigmoid_ballistic and fitting_functions\n",
    "    - With free $\\alpha$. \n",
    "\n",
    "Then, the notebook will use those functions as we follow the steps outlined just above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Importing scripts and data\n",
    "Same kind of code as in other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from time import perf_counter  # For timing\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scripts for data importation\n",
    "from ltspcyt.scripts.adapt_dataframes import set_standard_order, sort_SI_column\n",
    "from ltspcyt.scripts.neural_network import import_WT_output\n",
    "from ltspcyt.scripts.latent_space import import_mutant_output\n",
    "\n",
    "# Scripts for reconstruction, using distinct functions for distinct methods. \n",
    "from ltspcyt.scripts.reconstruction import (train_reconstruction, plot_recon_true, \n",
    "    compute_latent_curves, fit_param_distrib_kdes, ScalerKernelDensity, sample_from_kdes)\n",
    "\n",
    "# Scripts for curve fitting\n",
    "from ltspcyt.scripts.sigmoid_ballistic import (\n",
    "    return_param_and_fitted_latentspace_dfs, sigmoid_conc_full_freealpha, \n",
    "    ballistic_sigmoid_freealpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Import all data\n",
    "Remove unwanted levels, normalize the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wt = import_WT_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmaxfile = os.path.join(\"data\", \"trained-networks\", \"min_max-thomasRecommendedTraining.hdf\")\n",
    "df_min = pd.read_hdf(minmaxfile, key=\"df_min\")\n",
    "df_max = pd.read_hdf(minmaxfile, key=\"df_max\")\n",
    "df_min, df_max = df_min.xs(\"integral\", level=\"Feature\"), df_max.xs(\"integral\", level=\"Feature\")\n",
    "\n",
    "# Projection matrix\n",
    "P = np.load(os.path.join(\"data\", \"trained-networks\", \"mlp_input_weights-thomasRecommendedTraining.npy\")).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peptides = [\"N4\", \"Q4\", \"T4\", \"V4\", \"G4\", \"E1\", \"A2\", \"Y3\", \"A8\", \"Q7\"]\n",
    "concentrations = [\"1uM\", \"100nM\", \"10nM\", \"1nM\"]\n",
    "cytokines = df_min.index.get_level_values(\"Cytokine\")\n",
    "times = np.arange(0, 69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the desired cytokines, times, and T cell number\n",
    "df_wt = df_wt.unstack(\"Time\").loc[:, (slice(None), cytokines, times)].stack(\"Time\")\n",
    "\n",
    "# Rescale and project each feature, but do not offset (don't use MLP's intercepts)\n",
    "proj_dfs = []\n",
    "feat_keys = [\"integral\", \"concentration\", \"derivative\"]\n",
    "cols = pd.Index([\"Node 1\", \"Node 2\"], name=\"Node\", copy=True)\n",
    "print(P.T)\n",
    "\n",
    "for typ in feat_keys:\n",
    "    # Rescale with the training min and max\n",
    "    if typ == \"integral\":\n",
    "        df_wt[typ] = (df_wt[typ] - df_min)/(df_max - df_min)\n",
    "    else:   # for conc and deriv, the constant rescaling term disappears. \n",
    "        df_wt[typ] = df_wt[typ]/(df_max - df_min)\n",
    "    df_temp = pd.DataFrame(np.dot(df_wt[typ], P.T), index=df_wt[typ].index, columns=cols)\n",
    "    proj_dfs.append(df_temp)\n",
    "df_proj = pd.concat(proj_dfs, axis=1, names=[\"Feature\"], keys=feat_keys)\n",
    "del proj_dfs, cols, feat_keys  # temporary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove different T cell numbers\n",
    "tcellnum = \"100k\"\n",
    "df_wt = df_wt.xs(tcellnum, level=\"TCellNumber\", axis=0, drop_level=True)\n",
    "df_proj = df_proj.xs(tcellnum, level=\"TCellNumber\", axis=0, drop_level=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Select training and testing datasets\n",
    "Here, we don't remove A2 and Y3 from the reconstruction optimization (training) data, because the goal is to have reconstructions as good as possible and generate realistic cytokine trajectories from the model, and including as many peptides as possible helps. We do not have to show that we can reconstruct new peptides not previously seen in training: we already did that in the notebooks `reconstruct_cytokines_fromLSmodel_pvalues.ipynb` and `reconstruct_cytokines_fromLSdata.ipynb`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep multiple datasets to populate latent space better\n",
    "# Mix datasets with old and new protocols, because IL-6 is low in new ones, for instance. \n",
    "subset_exp = [\n",
    "    \"PeptideComparison_1\", \"PeptideComparison_2\", \"PeptideComparison_3\", \n",
    "    \"PeptideComparison_4\", \"PeptideComparison_5\", \"PeptideComparison_6\", \"PeptideComparison_7\",\n",
    "    \"TCellNumber_1\", \"Activation_1\"\n",
    "]\n",
    "subset_exp2 = [\"HighMI_1-\" + str(i) for i in range(1, 5)]\n",
    "\n",
    "# One can check that the results are basically the same when training the decoder on HighMI_1\n",
    "# by using subset_exp2 here\n",
    "df_wt_train = df_wt.loc[subset_exp]\n",
    "df_proj_train = df_proj.loc[subset_exp]\n",
    "df_wt_train = df_wt_train.loc[df_wt_train.index.isin(concentrations, level=\"Concentration\")]\n",
    "df_proj_train = df_proj_train.loc[df_proj_train.index.isin(concentrations, level=\"Concentration\")]\n",
    "\n",
    "df_wt_kde = df_wt.loc[subset_exp]\n",
    "df_proj_kde = df_proj.loc[subset_exp]\n",
    "df_wt_kde = df_wt_kde.loc[df_wt_kde.index.isin(concentrations, level=\"Concentration\")]\n",
    "df_proj_kde = df_proj_kde.loc[df_proj_kde.index.isin(concentrations, level=\"Concentration\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train the reconstruction function\n",
    "Also compute the reconstructed cytokines for both the test and training data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the reconstruction matrix, based on reconstructing integrals\n",
    "feature = \"concentration\"\n",
    "model_type = \"mixed_quad\"\n",
    "\n",
    "modelargs = {\"which_to_square\":[0, 1]}\n",
    "\n",
    "# Add some arbitrary features. \n",
    "# Try exponentials\n",
    "tanh_norm_factors = df_proj_train[\"integral\"].mean(axis=0)\n",
    "print(tanh_norm_factors)\n",
    "\n",
    "df_proj_train = pd.concat([df_proj_train[\"concentration\"], np.tanh(df_proj_train[\"integral\"] / tanh_norm_factors)], \n",
    "                           keys=[\"concentration\", \"tanh integral\"], names=[\"Feature\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe, score = train_reconstruction(df_proj_train, df_wt_train, feature=feature, \n",
    "                                   method=model_type, model_args=modelargs, do_scale_out=False)\n",
    "print(score)\n",
    "\n",
    "print(pipe[model_type].regressor_.Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remark\n",
    "We could also skip this step, and just import a pre-trained set of reconstruction coefficients, or in my case, a pickled reconstruction pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Fit the ballistic model to train data\n",
    "This is standard, we fit the integrals and rescale time by $\\tilde{t} = 20 h$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choice of fitting hyperparameters\n",
    "fit_vars={\"Constant velocity\":[\"v0\",\"t0\",\"theta\",\"vt\"],\"Constant force\":[\"F\",\"t0\",\"theta\",\"vt\"],\n",
    "         \"Sigmoid\":[\"a0\", \"tau0\", \"theta\", \"v1\", \"gamma\"], \n",
    "         \"Sigmoid_freealpha\":[\"a0\", \"tau0\", \"theta\", \"v1\", \"alpha\", \"beta\"]}\n",
    "fit = \"Sigmoid_freealpha\"\n",
    "regul_rate = 0.4\n",
    "tscale = 20.  # Rescaling of time for nicer parameter ranges\n",
    "name_specs = \"{}20_reg{}\".format(fit, str(round(regul_rate, 2)).replace(\".\", \"\"))\n",
    "\n",
    "# Fit the integrals\n",
    "start_time = perf_counter()\n",
    "\n",
    "ret = return_param_and_fitted_latentspace_dfs(\n",
    "    df_proj_kde.xs(\"integral\", level=\"Feature\", axis=1), \n",
    "    fit, reg_rate=regul_rate, time_scale=tscale)\n",
    "df_params, df_compare, df_hess, df_v2v1 = ret\n",
    "\n",
    "end_t = perf_counter()\n",
    "print(\"Time to fit: \", perf_counter() - start_time)\n",
    "del start_time\n",
    "\n",
    "nparameters = len(fit_vars[fit])\n",
    "print(df_hess.median())\n",
    "# The concentrations in df_compare should be good, because they are dN_i / dt computed numerically\n",
    "# so we don't have to worry about the magnitude of a_0, v_i. However, we want to generate the curves ourselves\n",
    "# with our equations, so we call the n_i function and take care of scaling as below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual inspection of the fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = subset_exp[-1]  # Select the data set to plot here\n",
    "tcellnum = \"100k\"\n",
    "df_compare_sel = df_compare.xs(dataset, level=\"Data\", axis=0)\n",
    "print(df_compare_sel.index.names)\n",
    "df_compare_sel.columns.names = [\"Node\"]\n",
    "data=df_compare_sel.loc[(peptides,concentrations,slice(None),slice(None),\"concentration\"),:]\n",
    "h=sns.relplot(data=data.stack().reset_index(),x=\"Time\",y=0,kind=\"line\",sort=False,\n",
    "            hue=\"Peptide\",hue_order=peptides,\n",
    "            col=\"Concentration\",col_order=concentrations, row=\"Node\",\n",
    "            style=\"Processing type\", height=3.25)\n",
    "#h.fig.tight_layout()\n",
    "# h.fig.savefig(os.path.join(\"figures\", \"fit\", \"concentrations_{}_{}.pdf\".format(name_specs, dataset)))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_params_sel = df_params.xs(tcellnum, level=\"TCellNumber\", axis=0)\n",
    "pep_order = [a for a in peptides if a in df_params.index.get_level_values(\"Peptide\").unique()]\n",
    "h = sns.pairplot(data=df_params.reset_index(), vars=[\"a0\", \"tau0\", \"theta\", \"v1\", \"alpha\", \"beta\"], \n",
    "                 hue=\"Peptide\", hue_order=pep_order)\n",
    "legend = h.legend\n",
    "\n",
    "#h.fig.savefig(os.path.join(\"figures\", \"fit\", \"pairplot_{}_selectdata.pdf\".format(name_specs)), \n",
    "#    transparent=True, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fit KDEs to the parameter distributions and sample from them\n",
    "This will then allow us in 5) to sample from those distributions and generate completely synthetic time courses. \n",
    "\n",
    "We need to make sure that all parameters have a comparable scale, otherwise the bandwidth may be way too large for some parameters and lead to completely crazy values. This happens for $v_1$, which has a scale 10x smaller than a_0, and  we can get $v_1$ way too large because of that.  So instead of just KDEs, use a pipeline with first a pre-fit (on all data) standard scaler where mins and maxs are the fitting bounds on the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit KDEs\n",
    "print(df_params.columns)\n",
    "dict_kdes, v2v1_kde = fit_param_distrib_kdes(df_params[fit_vars[fit]], df_v2v1, group_lvls=[\"Peptide\"])\n",
    "# Also, get a KDE of v2/v1 ratios, sample it for each peptide\n",
    "print(dict_kdes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_params_synth = sample_from_kdes(dict_kdes, fit_vars[fit], fit, {a:4 for a in dict_kdes.keys()}, seed=13069)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_params_sel = df_params.xs(tcellnum, level=\"TCellNumber\", axis=0)\n",
    "pep_order = [a for a in peptides if a in df_params.index.get_level_values(\"Peptide\").unique()]\n",
    "\n",
    "h = sns.pairplot(data=df_params_synth.reset_index(), vars=[\"a0\", \"tau0\", \"theta\", \"v1\"], \n",
    "                 hue=\"Peptide\", hue_order=pep_order)\n",
    "legend = h.legend\n",
    "\n",
    "#h.fig.savefig(os.path.join(\"figures\", \"fits\", \"pairplot_{}_selectdata.pdf\".format(name_specs)), \n",
    "#    transparent=True, bbox_extra_artists=(legend,), bbox_inches='tight')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final slopes sample\n",
    "ser_v2v1_synth = pd.Series(v2v1_kde.sample(len(df_params_synth.index))[:, 0], \n",
    "                          index=df_params_synth.index, name=\"v2v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Compute ballistic curves for sampled parameters\n",
    "This part is the trickiest: we fitted $N_i(t')$, where $t' = t/\\tilde{t}$, $\\tilde{t} = 20 $ h (the time scale). Now, we want $n_i(t) = \\frac{d N_i}{dt} = \\frac{d t'}{dt} \\frac{d N_i(t')}{d t'} = \\frac{1}{\\tilde{t}} n_i(t', a_0', \\ldots)$, where $n_i(t', a_0', \\ldots)$ is the formal function $n_i(t)$ called with $t'$ and parameters fitted for $N_i(t')$, instead of $t$: same functional form, different scale of variables. We need to compensate this by dividing by $\\tilde{t}$. This is taken care of in the function compute_latent_curves. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new df_compare, by concatenation.\n",
    "df_latent_synth = compute_latent_curves(df_params_synth, ser_v2v1_synth, tanh_norm_factors, times,\n",
    "    model=\"Sigmoid_freealpha\", tsc=tscale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_params_synth.loc[\"N4\", \"tau0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h=sns.relplot(data=df_latent_synth.xs(feature, level=\"Feature\", axis=1).stack().reset_index(), \n",
    "            x=\"Time\", y=0, kind=\"line\", sort=False, hue=\"Peptide\", hue_order=peptides,\n",
    "            col=\"Replicate\", row=\"Node\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reconstruct cytokines from generated curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recon_synth = pd.DataFrame(pipe.predict(df_latent_synth), index=df_latent_synth.index, \n",
    "                             columns=df_wt_kde.xs(feature, axis=1, level=\"Feature\", drop_level=False).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_recon_synth = pd.concat([df_recon_synth], keys=[\"Synthetic\"], names=[\"Data\"]+df_recon_synth.index.names)\n",
    "df_recon_synth.index = df_recon_synth.index.rename(\"Concentration\", \"Replicate\")\n",
    "df_recon_synth.index = df_recon_synth.index.set_levels([\"1uM\", \"100nM\", \"10nM\", \"1nM\"], level=\"Concentration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Compare the generated cytokines to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#figlist = plot_recon_true(df_recon_synth, df_recon_synth, feature=feature, sharey=True, do_legend=False, \n",
    "#                          palette=pep_palette, pept=peptides)\n",
    "dset = subset_exp[1]\n",
    "df_both = pd.concat([df_wt_kde.xs(dset, level=\"Data\", axis=0), df_recon_synth], \n",
    "                    axis=0, keys=[\"HighMI\", \"Synth\"], names=[\"Data\"])\n",
    "\n",
    "with sns.plotting_context(\"notebook\", font_scale=0.75):\n",
    "    h = sns.relplot(data=df_both.stack().reset_index(),x=\"Time\",y=\"concentration\", size=\"Concentration\",\n",
    "                kind=\"line\",sort=False, hue=\"Peptide\", hue_order=pep_order, style=\"Data\", \n",
    "                style_order=[\"Synth\", \"HighMI\"],\n",
    "                row=\"Cytokine\", col=\"Peptide\", col_order=[\"N4\", \"A2\", \"Y3\", \"Q4\", \"T4\", \"V4\"], \n",
    "               height=2.5, aspect=1)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving useful results\n",
    "- The dictionary of fitted ScalerKernelDensity instances: will need to import the ScalerKernelDensity class and StandardScaler before reading the pickled object. \n",
    "- The Pipeline used for reconstruction. Will need to import the QuadraticRegression class before reading the pickled object. \n",
    "- The scaling coefficients in the tanh (pd.DataFrame). \n",
    "- The sampled parameter dataframe (pd.DataFrame)\n",
    "- The reconstructed cytokines from the sampled dataframe (pd.DataFrame). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip reconstructed cytokines (if we judge they are reasonable above) to remove slightly negative values\n",
    "df_recon_synth.clip(lower=0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = \"HighMI_1\" if np.all([a.startswith(\"HighMI_1\") for a in subset_exp]) else \"selectdata\"\n",
    "folder = os.path.join(\"results\", \"reconstruction\")\n",
    "# Pipeline and KDEs\n",
    "with open(os.path.join(folder, \"scalerkde_dict_sigmoid_freealpha_{}.pkl\".format(option)), \"wb\") as hd:\n",
    "    pickle.dump(dict_kdes, hd)\n",
    "with open(os.path.join(folder, \"v2v1_kde_sigmoid_freealpha_{}.pkl\".format(option)), \"wb\") as hd:\n",
    "    pickle.dump(v2v1_kde, hd)\n",
    "\n",
    "with open(os.path.join(folder, \"quadratic_tanh_pipeline_{}.pkl\".format(option)), \"wb\") as hd:\n",
    "    pickle.dump(pipe, hd)\n",
    "tanh_norm_factors.to_hdf(os.path.join(folder, \"tanh_norm_factors_integrals_{}.hdf\".format(option)), key=\"tanh_norm\")\n",
    "\n",
    "# Generated parameters\n",
    "df_params_synth.to_hdf(os.path.join(folder, \"df_params_synth_sigmoid_freealpha_{}.hdf\".format(option)), key=\"df_params_synth\")\n",
    "ser_v2v1_synth.to_hdf(os.path.join(folder, \"ser_v2v1_synth_{}.hdf\".format(option)), key=\"ser_v2v1_synth\")\n",
    "\n",
    "# Generated data (clipped to remove negative values)\n",
    "df_recon_synth.to_hdf(os.path.join(folder, \"df_recon_synth_sigmoid_freealpha_{}.hdf\".format(option)), key=\"df_recon_synth\")\n",
    "df_latent_synth.to_hdf(os.path.join(folder, \"df_latent_synth_sigmoid_freealpha_{}.hdf\".format(option)), key=\"df_latent_synth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
